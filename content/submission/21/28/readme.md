---
en:
  sequence: 28
  body: >-
    <﻿!--Start Fragment-->


    L﻿ink to Submitted Work: N/A


    Our project examines pedestrian behavior in public spaces, drawing on the observational work of urbanist William Whyte from the 1980s. Using computer vision, we compared footage of public spaces in New York, Philadelphia, and Boston from the 1980s with more recent footage from 2010. Our findings reveal that pedestrians are walking faster and lingering less, signaling a shift from streets as social spaces to more functional thoroughfares.


    We are translating this pedestrian behavioral data into a “musical score,” enabling visitors to hear the rhythms of public life across time and place. This score—both visual and sonic—invites audiences to reflect on how public spaces shape and are shaped by collective human intelligence. The musical interpretation also draws inspiration from composer John Cage, whose work explored the interplay between city sounds and artistic expression.


    The visualization and sonification are designed to be exhibited in public spaces, including the four original plazas studied by Whyte and Project for Public Spaces. For each space, the music and visual score can be adapted to its unique tempo and social dynamics. We are experimenting with AI-based generative music techniques such as Music Transformer: Generating Music with Long-Term Structure.


    This project envisions intelligence not only in artificial systems but in the emergent behaviors of everyday urban life—how people move, pause, and interact. By making those patterns audible and visible, we aim to spark conversation about the evolving nature of public space.


    <﻿!--End Fragment-->
  externalLink: https://senseable.mit.edu/
  references:
    - "<﻿!--Start Fragment--> Note: the visualization and music of this project
      is still undergoing and funded by several resources. The final link will
      be live at Senseable City Lab homepage (https://senseable.mit.edu/) around
      Summer 2025 when the research paper is online. <!--End Fragment-->"
    - >-
      <﻿!--Start Fragment--> Funded by Council for the Arts at MIT (CAMIT) and
      MIT Open Space Programming to do an interactive exhibit that translates
      data of pedestrian behavior and interactions in public spaces on campus
      into choreographed dance and music.

      https://arts.mit.edu/spring-2025-camit-grant-recipients/ <!--End Fragment-->
    - >-
      <﻿!--Start Fragment--> The pedestrian research data are from research
      paper under revision at PNAS, preprint on NBER:

      https://www.nber.org/papers/w33185 <!--End Fragment-->
    - >-
      <﻿!--Start Fragment--> The research and video are presented on the
      ARTIFICIAL section at the Arsenale in Venice Biennale 2025: Intelligens.
      Natural. Artificial. Collective.

      https://www.labiennale.org/en/architecture/2025/artificial/eyes-street <!--End Fragment-->
    - >-
      <﻿!--Start Fragment--> Video folder:

      https://drive.google.com/drive/folders/1irUoUV0-ctleT5Cv9FF_9rDQLH3SZ885?usp=sharing <!--End Fragment-->
    - >-
      <﻿!--Start Fragment--> Several sonification/music generation techniques
      the artists are experimenting with include:

      Music Transformer: Generating Music with Long-Term Structure

      https://magenta.tensorflow.org/music-transformer <!--End Fragment-->
    - |-
      <﻿!--Start Fragment--> Project for Public Spaces:
      https://www.pps.org/ <!--End Fragment-->
  makers:
    - jingrong-zhang/readme
    - fábio-duarte/readme
  title: Urban Choreographies
  image:
    sm: urbanchoreographiessm.png
    med: urbanchoreographiessm.png
    lg: urbanchoreographiessm.png
  year: 2025
  iteration: 21
es: {}
zh: {}
fr: {}
pt: {}
de: {}
pl: {}
---
