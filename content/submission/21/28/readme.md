---
en:
  sequence: 28
  body: >-
    L﻿ink to Submitted Work: N/A


    Our project examines pedestrian behavior in public spaces, drawing on the observational work of urbanist William Whyte from the 1980s. Using computer vision, we compared footage of public spaces in New York, Philadelphia, and Boston from the 1980s with more recent footage from 2010. Our findings reveal that pedestrians are walking faster and lingering less, signaling a shift from streets as social spaces to more functional thoroughfares.


    We are translating this pedestrian behavioral data into a “musical score,” enabling visitors to hear the rhythms of public life across time and place. This score—both visual and sonic—invites audiences to reflect on how public spaces shape and are shaped by collective human intelligence. The musical interpretation also draws inspiration from composer John Cage, whose work explored the interplay between city sounds and artistic expression.


    The visualization and sonification are designed to be exhibited in public spaces, including the four original plazas studied by Whyte and Project for Public Spaces. For each space, the music and visual score can be adapted to its unique tempo and social dynamics. We are experimenting with AI-based generative music techniques such as Music Transformer: Generating Music with Long-Term Structure.


    This project envisions intelligence not only in artificial systems but in the emergent behaviors of everyday urban life—how people move, pause, and interact. By making those patterns audible and visible, we aim to spark conversation about the evolving nature of public space.
  externalLink: https://senseable.mit.edu/
  references:
    - "Note: the visualization and music of this project is still undergoing and
      funded by several resources. The final link will be live at Senseable City
      Lab homepage (https://senseable.mit.edu/) around Summer 2025 when the
      research paper is online."
    - >-
      Funded by Council for the Arts at MIT (CAMIT) and MIT Open Space
      Programming to do an interactive exhibit that translates data of
      pedestrian behavior and interactions in public spaces on campus into
      choreographed dance and music.

      https://arts.mit.edu/spring-2025-camit-grant-recipients/
    - >-
      The pedestrian research data are from research paper under revision at
      PNAS, preprint on NBER:

      https://www.nber.org/papers/w33185
    - >-
      The research and video are presented on the ARTIFICIAL section at the
      Arsenale in Venice Biennale 2025: Intelligens. Natural. Artificial.
      Collective.

      https://www.labiennale.org/en/architecture/2025/artificial/eyes-street
    - >-
      Video folder:

      https://drive.google.com/drive/folders/1irUoUV0-ctleT5Cv9FF_9rDQLH3SZ885?usp=sharing
    - >-
      Several sonification/music generation techniques the artists are
      experimenting with include:

      Music Transformer: Generating Music with Long-Term Structure

      https://magenta.tensorflow.org/music-transformer
    - |-
      Project for Public Spaces:
      https://www.pps.org/
  makers:
    - jingrong-zhang/readme
    - fábio-duarte/readme
  title: Urban Choreographies
  image:
    sm: urbanchoreographiessm.png
    med: urbanchoreographiessm.png
    lg: urbanchoreographiessm.png
  year: 2025
  iteration: 21
es:
  makers:
    - jingrong-zhang/readme
    - fábio-duarte/readme
zh: {}
fr: {}
pt: {}
de: {}
pl: {}
---
