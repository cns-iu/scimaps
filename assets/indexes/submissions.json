[
  {
    "en": {
      "iteration": 18,
      "sequence": 1,
      "year": 2022,
      "title": "Atlas of Inequality",
      "body": "Authors: Esteban Moro\n\nLink to Macoscope: [Inequality.media.mit.edu](http://inequality.media.mit.edu) \n\nAbstract: \n\nEconomic inequality isn't just limited to neighborhoods. The restaurants, stores, and other places we visit in cities are all unequal in their own way.\n\nThe Atlas of Inequality shows the income inequality of people who visit different places in cities around the U.S. It uses aggregated anonymous location data from digital devices to estimate people's incomes and where they spend their time.\n\nUsing that data, we've made our own place inequality metric to capture how unequal the incomes of visitors to each place are. Economic inequality isn't just limited to neighborhoods, it's part of the places you visit every day.",
      "references": [
        "<https://www.nature.com/articles/s41467-021-24899-8>"
      ],
      "makers": [
        "esteban-moro/readme"
      ],
      "image": {
        "sm": "screenshot-2022-03-03-at-10.44.08-am.png",
        "med": "screenshot-2022-03-03-at-10.44.08-am.png",
        "lg": "screenshot-2022-03-03-at-10.44.08-am.png"
      }
    },
    "es": {
      "makers": [
        "esteban-moro/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 18,
      "sequence": 2,
      "year": 2019,
      "title": "Excellence Maps: The new release, including reader impact of science institutions worldwide",
      "body": "Authors: Lutz Bornmann, Rüdiger Mutz, Moritz Stefaner, Felix de Moya Angeon, Robin Haunschild, Mirko Clemente\n\n\n\nLink to Macroscope: <https://www.excellencemapping.net/>\n\n\n\nAbstract: \n\nIn over five years, we have published several releases of the www.excellencemapping.net tool revealing (clusters of) excellent institutions worldwide based on citation data. With the new release, a completely revised tool has been published. It is not only based on citation data (bibliometrics) but also Mendeley data (altmetrics). Thus, the institutional impact measurement of the tool has been expanded by focusing on additional status groups besides researchers, such as students and librarians. The excellencemapping.net is the first university ranking that includes altmetrics data to visualize the impact of institutional research beyond research itself. For example, the new release facilitates that those institutions worldwide can be identified with the highest impact of research on students. In the new release, the visualization of the data has been completely updated by improving the operability for the user and including new features, such as institutional profile pages.",
      "makers": [
        "lutz-bornmann/readme",
        "rudiger-mutz/readme",
        "moritz-stefaner/readme",
        "robin-haunschild/readme",
        "mirko-clemente/readme",
        "felix-de-moya-angeon/readme"
      ],
      "image": {
        "sm": "screenshot-2022-03-03-at-10.48.42-am.png",
        "med": "screenshot-2022-03-03-at-10.48.42-am.png",
        "lg": "screenshot-2022-03-03-at-10.48.42-am.png"
      },
      "references": [
        "Recent paper introducing the new release of excellencemapping.net: https://link.springer.com/article/10.1007/s11192-021-04141-4\n\nRelated site: <http://www.excellence-networks.net/>"
      ]
    },
    "es": {
      "makers": [
        "lutz-bornmann/readme",
        "rudiger-mutz/readme",
        "moritz-stefaner/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 18,
      "sequence": 3,
      "year": 2022,
      "title": "The IPCC Interactive Atlas",
      "body": "Authors: The IPCC WGI Atlas team \n\nLink to Macroscope: <https://interactive-atlas.ipcc.ch/regional-information> \n\nAbstract:\n\nThe IPCC Interactive Atlas is an innovative component of the Sixth Assessment Report (AR6) of the Intergovernmental Panel on Climate Change (IPCC) supporting and extending the assessment provided in the report –in particular regional assessment– and improving transparency and re-use of climate information through the implementation of FAIR principles (Iturbide et al. 2021). The Interactive Atlas allows exploring over 25 essential climate atmospheric and oceanic variables (such as temperature, precipitation, wind and sea surface temperature, sea level rise, sea ice, and pH) and impact-relevant indices underpinning the report. These variable are computed for both multi-model global (CMIP6) and regional (CORDEX) climate change projections and can be analyzed along different climate change dimensions (future periods across scenarios or global warming levels) using a number of  innovative interactive data visualizations, such as global and regional maps,  ensemble time series, climate stripes, global warming level plots, etc.. A strong effort in data visualization has been made to communicate the uncertainty of the models, through hatching of the maps, and showing always an ensemble of climate models (instead of single-model results).\n\nThe data processing was challenging and time-consuming (data access, curation index calculation, standardization and metadata provision, etc.), requiring over 1.5 million hours of computing time. The processing of climate data handled several hundred TBs of initial information, and distilled it down to a total of 2 TB, that is the final dataset handled by the Interactive Atlas.\n\nTwo types of users were targeted. First, the IPCC researchers and practitioners, since the tool was issued as part of IPCC AR6. Second, education, media and the general public. . For the latter, a simple interface (climate futures) was deployed oriented to global warming levels, so users are directly confronted with the different policy relevant choices (our possible future worlds at 1.5º, 2º or 3º).",
      "makers": [
        "the-ipcc-wgi-atlas-team/readme"
      ],
      "references": [
        "IPCC, 2021: Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change \\[Masson-Delmotte, V., P. Zhai, A. Pirani, S.L. Connors, C. Péan, S. Berger, N. Caud, Y. Chen, L. Goldfarb, M.I. Gomis, M. Huang, K. Leitzell, E. Lonnoy, J.B.R. Matthews, T.K. Maycock, T. Waterfield, O. Yelekçi, R. Yu, and B. Zhou (eds.)]. Cambridge University Press. In Press.\n\nGutiérrez, J.M., R.G. Jones, G.T. Narisma, L.M. Alves, M. Amjad, I.V. Gorodetskaya, M. Grose, N.A.B. Klutse, S. Krakovska, J. Li, D. Martínez-Castro, L.O. Mearns, S.H. Mernild, T. Ngo-Duc, B. van den Hurk, and J.-H. Yoon, 2021: Atlas. In Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change \\[Masson-Delmotte, V., P. Zhai, A. Pirani, S.L. Connors, C. Péan, S. Berger, N. Caud, Y. Chen, L. Goldfarb, M.I. Gomis, M. Huang, K. Leitzell, E. Lonnoy, J.B.R. Matthews, T.K. Maycock, T. Waterfield, O. Yelekçi, R. Yu, and B. Zhou (eds.)]. Available from http://interactive-atlas.ipcc.ch \n\nIturbide et al. 2019 “The R-based climate4R open framework for reproducible climate data access and post-processing”, Environmental Modelling & Software, 111, 42-54, https://doi.org/10.1016/j.envsoft.2018.09.009\n\nIturbide, M. et al., 2020: An update of IPCC climate reference regions for subcontinental analysis of climate model data: definition and aggregated datasets. Earth System Science Data, 12(4), 2959–2970, doi:10.5194/essd- 12-2959-2020.\n\nIturbide, M. et al., 2021: Repository supporting the implementation of FAIR principles in the IPCC-WG1 Interactive Atlas. Zenodo. http://doi.org/10.5281/zenodo.5171760. https://github.com/IPCC-WG1/Atlas"
      ],
      "image": {
        "sm": "screenshot-2022-03-03-at-11.05.16-am.png",
        "med": "screenshot-2022-03-03-at-11.05.16-am.png",
        "lg": "screenshot-2022-03-03-at-11.05.16-am.png"
      }
    },
    "es": {
      "makers": [
        "abin-abraham/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 18,
      "sequence": 4,
      "year": 2020,
      "title": "Cracking the Mystery of Egg Shape ",
      "body": "Authors: Sarah Crespi, Jia You\n\nLink to Macroscope: <https://vis.sciencemag.org/eggs/>  or <https://visdev.sciencemag.org/eggs/>  \n\nAbstract:\n\nUser group: This piece is for the science-interested public, students, and educators\n\nData used: The macroscope is based on a research paper published in Science. The paper uses digitized photographs of eggs from 1400 bird species to show what factors are important in determining the shape of bird eggs. https://www.science.org/doi/10.1126/science.aaj1945\n\nData analysis: The researchers measured the eggs in the photos and used different approaches to determine how different egg shapes are formed in a bird and also which egg shapes correlate with other characteristics of a bird species, such as wing type.\n\nViz techniques applied: \n\nPacked circle diagram to show how many eggs, bird species, and orders were included in the data \n\nGraph to show how bird egg shapes were characterized in the study\n\nEgg photographs used as data in the paper\n\nAnimations of how different factors might influence egg shape  \n\nGraphs showing the relationship between egg shape and flight style \n\nMain insights gained: CRACKING THE MYSTERY OF EGG SHAPE follows researchers’ efforts to solve a long-standing mystery in biology: Why are eggs shaped differently from species to species? The work visualizes results from the analysis of a data set of 1400 species’ eggs, cataloged in a repository of egg photographs collected over the course of 100 years. The site exposes viewers to the diversity of birds, egg shapes, and the mechanisms by which eggs are formed. It also demonstrates how science can use data from historical sources in new ways.",
      "image": {
        "sm": "screenshot-2022-03-03-at-11.07.50-am.png",
        "med": "screenshot-2022-03-03-at-11.07.50-am.png",
        "lg": "screenshot-2022-03-03-at-11.07.50-am.png"
      },
      "makers": [
        "sarah-crespi/readme",
        "jia-you/readme"
      ],
      "references": [
        "https://www.science.org/doi/10.1126/science.aaj1945 Egg collection at Museum of Vertebrate Zoology https://mvz.berkeley.edu/mvzegg/"
      ]
    },
    "es": {
      "makers": [
        "abin-abraham/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 18,
      "sequence": 6,
      "year": 2020,
      "title": "The Social Determinants of Health Visualization Tool",
      "body": "Author: Chang Zhao, Peter Herman\n\nLink to Macroscope: <https://sdoh-visualization-tool.norc.org/> \n\nAbstract: \n\nThe Social Determinants of Health Visualization Tool is an R Shiny application that provides a user-friendly platform for public health researchers, policy makers and the public to examine spatial-temporal patterns of and relationships between social determinants of health (SDOH) measures known to influence health outcomes at multi-scales in the U.S.\n\nThis visualization tool uses the Agency for Healthcare Research and Quality (AHRQ) SDOH beta database, which was curated from multiple Federal and publicly available data sources. Measures in the AHRQ SDOH database correspond to five key SDOH domains: social context (e.g., age, race/ethnicity), economic context (e.g., income, poverty), education, physical infrastructure (e.g., crime, environment), and healthcare context (e.g., health access, quality, behaviors). SDOH measures were collected for multiple years at two geographic levels in the U.S., i.e., county (324 variables, from 2009 to 2018), ZCTA (159 variables, from 2011 to 2018).\n\nUsing maps created by the Mapbox GL JavaScript library, and associated scatterplots, summary tables, and graphs created by R Shiny, this tool can display a wide range of SDOH measures at scale. There are three key elements of the tool: \n\n•        The Interactive Map tab renders county and ZCTA-level maps with options to select year and search for specific SDOH measure sorted by SDOH domain and topic. The zoom-in function allows for quick examination of SDOH in individual state and county.\n\n•        The Change Analysis tab provides side by side synchronized maps, a scatterplot with Pearson correlation test, a multi-group histogram and a summary table, allowing users to investigate temporal changes of a SDOH measure.\n\n•        The Pattern Comparisons tab further enables users to identify spatial associations and statistical relationships between different SDOH measures. \n\nWith these features, this visualization tool can be used to obtain critical insights into the spatial-temporal patterns in and associations between SDOH measures, thereby informing public health research and practice.",
      "image": {
        "sm": "screenshot-2022-03-03-at-11.13.55-am.png",
        "med": "screenshot-2022-03-03-at-11.13.55-am.png",
        "lg": "screenshot-2022-03-03-at-11.13.55-am.png"
      },
      "makers": [
        "peter-herman/readme",
        "chang-zhao/readme"
      ],
      "references": [
        "<https://www.ahrq.gov/sdoh/data-analytics/sdoh-data.html>"
      ]
    },
    "es": {
      "makers": [
        "peter-herman/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 18,
      "sequence": 7,
      "year": 2020,
      "title": "Star Mapper",
      "body": "Author: Jan Willem Tulp \n\nLink to Macroscope: <https://sci.esa.int/star_mapper/> \n\nAbstract:\n\nESA Star Mapper is an interactive visualization that lets a science interested audience explore what you can find in a star map. The data used for this visualization is based on the Hipparcos mission, which has measured the details of over 100.000 stars, at the time the largest such a collection of data about stars. The project was conceived as a stepping stone to a follow up mission, Gaia, in which measurements of 1.5 billion stars have been collected. The visualization allows users to interactively explore various measurements of more than  60.000 of the full dataset, including a real-time sped up simulation of stellar motion in the browser.",
      "makers": [
        "jan-willem-tulp/readme"
      ],
      "image": {
        "sm": "screenshot-2022-03-03-at-11.15.02-am.png",
        "med": "screenshot-2022-03-03-at-11.15.02-am.png",
        "lg": "screenshot-2022-03-03-at-11.15.02-am.png"
      },
      "references": []
    },
    "es": {
      "makers": [
        "jan-willem-tulp/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 18,
      "sequence": 8,
      "year": 2020,
      "title": "Translation Spaces",
      "body": "Author: Philipp Hofeneder \n\nLink to Macroscope: <https://homepage.uni-graz.at/de/7/thematische-landkarten/>\n\nAbstract:\n\nResearch with a focus on Humanities; first, maps were drawn by hand, then worked out with the help of Illustrator; they consist of several layers (translator, one specific translational project, other translations of the translator); they allow us to reconstruct certain movement patterns and shed a new light on translation as a social activity. The topic of the following maps are the translators of a history of Russia (written by Nikolay Karamzin) and published between 1818 and 1829 in St. Petersburg. The five translators translated the text into German. \n\nDesigned, developed, and narrated by Kim Albrecht, Matthew Battles, Fabian Dinklage, and Sydney Lewis",
      "makers": [
        "philipp-hofeneder/readme"
      ],
      "image": {
        "sm": "screenshot-2022-03-03-at-11.15.35-am.png",
        "med": "screenshot-2022-03-03-at-11.15.35-am.png",
        "lg": "screenshot-2022-03-03-at-11.15.35-am.png"
      },
      "references": [
        "<https://homepage.uni-graz.at/de/philipp.hofeneder/habilitationsprojekt-a-cartography-of-translation/>"
      ]
    },
    "es": {
      "makers": [
        "philipp-hofeneder/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 1,
      "year": 2023,
      "title": "Info and Violence",
      "body": "Authors: Siori Kitajima\n\nLink to Macoscope: <https://182.patternbased.com/information-and-violence/>\n\nAbstract:  \n\n\"Information and Violence\" is an interactive data visualization that presents a timeline of major U.S. news headlines alongside monthly counts of gun sales and homicides from 2016 to 2021. Since 2016, the number of gun sales and homicides has been steadily increasing. By displaying them alongside the top news at the time, it aims to highlight the possibility of the psychological influence of fear and anxiety in society. Users can hover their cursor over the screen to view the gun sales and homicides data, and click on orange vertical lines to see the corresponding news headline.\n",
      "makers": [
        "siori-kitajima/readme"
      ],
      "references": [
        "HOMICIDES = From 2016 to 2019, the data is from CDC WONDER Data Archive (https://wonder.cdc.gov/). From 2020, since the data publishing by CDC has been paused since the COVID pandemic, we used the estimated monthly homicide numbers from the report published by CCJ National Commission on COVID-19 and Criminal Justice (https://cdn.ymaws.com/counciloncj.org/resource/resmgr/covid_commission/Year_End_Crime_Update_Design.pdf).\n\nGUN SALES = Brady's estimated monthly gun sales (https://brady-static.s3.amazonaws.com/NICSdata-year.pdf), which is calculated based on NICS's background check for firearm sales (https://www.fbi.gov/services/cjis/nics).\n\nNEWS EVENTS = Significant news events picked from Wikipedia's list of incidents in U.S. \n\n2016 in the United States (https://en.wikipedia.org/wiki/2016_in_the_United_States), \n\n2017 in the United States (https://en.wikipedia.org/wiki/2017_in_the_United_States), \n\n2018 in the United States (https://en.wikipedia.org/wiki/2018_in_the_United_States), \n\n2019 in the United States (https://en.wikipedia.org/wiki/2019_in_the_United_States), \n\n2020 in the United States (https://en.wikipedia.org/wiki/2020_in_the_United_States), \n\n2021 in the United States (https://en.wikipedia.org/wiki/2021_in_the_United_States).\n"
      ],
      "image": {
        "sm": "infoandviolence.png",
        "med": "infoandviolence.png",
        "lg": "infoandviolence.png"
      }
    },
    "es": {
      "makers": [
        "siori-kitajima/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 2,
      "year": 2023,
      "title": "Spain lives in flats: why we have built our cities vertically",
      "body": "Authors: Raúl Sánchez González, Analía Plaza\n\nLink to Macroscope: <https://especiales.eldiario.es/spain-lives-in-flats/>\n\nAbstract:  \n\n\"Spain lives in flats\" is an innovative and interactive journalistic project by elDiario.es that analyzes the footprint of more than 12 million buildings to map the height of Spanish cities in 3D to analyze their sustainability and impact on the current urban society. The investigation that includes narrative visualizations shows why Spain is one of the countries in the world that has built the highest and most dense cities compared to other countries. Two of every three Spaniards live in flats and apartments and only a third in single-family and semi-detached houses.\n\nWe created an R script that downloaded cadastral data and footprints of more than 12 million buildings. The total size of the downloaded files exceeded 200GB.\n\nThen, we processed and joined in R all downloaded files to have a single database with the details of all the buildings in Spain. For example, we verified the data of 12 million buildings, we calculated the height of every building from the part that occupies the most built volume in the plans and we extracted the date of the building from the start of construction. All these processes were repeated several times during the year due to the publication of corrections and allowed us for the first time in Spain to have a database and a map with the heights of all the buildings in the country. This project compiled for the first time in a single database the cartographic plans of all the buildings currently standing in the Spanish territory and proved that in Spain we live in tall buildings.\n\nWhen considering the structure of the story, we conceived it as a journey from the small anecdote to the big phenomenon. For this reason, the piece begins by telling the story of a building in A Coruña and then explaining vertical urbanism in Spain. This story works as a historical journey through each urban model developed in Spain contextualized with graphics and details of each moment.\n \nWe used R, Rstudio, QGIS and Tippecanoe for data compilation, processing and analysis. We had to do a lot of tests to compress the geographic information as much as possible to be able to map it on all devices. D3.js and Javascript were used for data visualization. Mapbox for mapping building footprint. HTML, Javascript and Scrollama for scrolling webpage.\n",
      "makers": [
        "raúl-sánchez-gonzález/readme",
        "analía-plaza/readme"
      ],
      "references": [
        "-﻿"
      ],
      "image": {
        "sm": "spainlivesinflats.png",
        "med": "spainlivesinflats.png",
        "lg": "spainlivesinflats.png"
      }
    },
    "es": {
      "makers": [
        "raúl-sánchez-gonzález/readme",
        "analía-plaza/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 3,
      "year": 2023,
      "title": "WorldViewR: Interactive Elevation Mapping and 3D Modeling Program",
      "body": "Authors: Jonathan Callura \n\nLink to Macroscope: <https://www.worldviewr.com>\n\nAbstract: \n\nThe WorldViewR web application provides a point and click interface that allows users to quickly render interactive 3D elevation maps. These models can be exported in various formats and they have been used for applications such as 3D printing and surface water flow modeling. \n",
      "makers": [
        "jonathan-callura/readme"
      ],
      "references": [
        "-﻿"
      ],
      "image": {
        "sm": "worldviewr.png",
        "med": "worldviewr.png",
        "lg": "worldviewr.png"
      }
    },
    "es": {
      "makers": [
        "jonathan-callura/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 4,
      "year": 2023,
      "title": "K-Means Clustering: An Explorable Explainer",
      "body": "Authors: Yi Zhe Ang\n\nLink to Macroscope: <https://k-means-explorable.vercel.app/>\n\nAbstract:\nThis interactive article presents a so-called “explorable explainer” of the k-means clustering algorithm. It attempts to push the envelope of how visual explanations can be designed on an interactive computational medium such as the web.\n\nThis article explains the algorithm using a “scrollytelling” piece. As the reader scrolls through the article, not only do the visuals animate and transition accordingly with the text, the reader is also prompted to interact with these geometric visualizations of the algorithm, supplementing their intuition through hands-on experimentation with its inner workings.\n\nAs the article progresses and introduces more concepts, it also slowly introduces corresponding knobs and dials that the reader can play around with. The article finally concludes with a dashboard or sandbox that comprises all the interactive controls that came before, giving the reader free rein to probe around and dissect the algorithm to improve their own understanding of its details as they please.\n",
      "makers": [
        "yi-zhe-ang/readme"
      ],
      "image": {
        "sm": "kmeansclustering.png",
        "med": "kmeansclustering.png",
        "lg": "kmeansclustering.png"
      },
      "references": [
        "- https://developers.google.com/machine-learning/clustering\n- https://scikit-learn.org/stable/modules/clustering.html#k-means\n- https://probml.github.io/pml-book/book1.html\n"
      ]
    },
    "es": {
      "makers": [
        "yi-zhe-ang/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 5,
      "year": 2020,
      "image": {
        "sm": "spatialequality.png",
        "med": "spatialequality.png",
        "lg": "spatialequality.png"
      },
      "title": "Spatial Equity NYC",
      "body": "Authors: Niko McGlashan, Sarah Williams, Daniela Coray, Kelly Fang, Enrique Casillas, Jari Prachasartta, Jiajie Li\n\nLink to Macroscope: <https://www.spatialequity.nyc/>\n\nAbstract: \n\nSpatial Equity NYC documents inequities in the ways that public space — including streets, sidewalks, and greenspaces — is designed, distributed, and accessed. Browse citywide data or search community profiles to learn how decisions about the use of public space lead to unequal outcomes and what you can do about it.\n",
      "makers": [
        "niko-mcglashan/readme",
        "sarah-williams/readme",
        "daniela-coray/readme",
        "kelly-fang/readme",
        "enrique-casillas/readme",
        "jari-prachasartta/readme",
        "jiajie-li/readme"
      ],
      "references": [
        "- Please check back with us in the coming weeks should we be accepted to include more relevant publications and conference proceedings.\n\n- Homepage for the Leventhal Center for Advanced Urbanism https://lcau.mit.edu/\n"
      ]
    },
    "es": {
      "makers": [
        "niko-mcglashan/readme",
        "sarah-williams/readme",
        "daniela-coray/readme",
        "kelly-fang/readme",
        "enrique-casillas/readme",
        "jari-prachasartta/readme",
        "jiajie-li/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 6,
      "year": 2023,
      "image": {
        "sm": "thenowmuseum.png",
        "med": "thenowmuseum.png",
        "lg": "thenowmuseum.png"
      },
      "title": "The Now.museum - The COVID19 museum timeline",
      "body": "Authors: Andrea Scharnhorst, Vyacheslav Tykhonov, Juliette Vion-Dury, Yves Rozenholc, \n\nLink to Macroscope: <http://info126.pharmacie.univ-paris5.fr:8095/timeline?q=covid>\n\nAbstract: \n\nWe propose a general toolbox for the Now.museum - a \"digital space\" dedicated to individual and collective memories of significant events in our societies - whose context is informed by the collection and conservation of artifacts publicly available on the web. \nClearly, Covid-19 and its global spreading belong to these significant events and could be documented by sharing digital collections across specific societal sectors, scientific domains, etc. However, a question is “how to capture its double spreading dimensionality spatially and in time”? Even if several retrospective initiatives exist, an integrative data curation approach and interface on information and testimonies is still missing. Moreover, relatively little attention was given to the ‘voices’ of the public, their testimonies, experiences, losses, and aspirations. \nFollowing the Open Science principle for science democratization, our macroscope – with its timeline interface -  answers these concerns with open data which provides context to individual experiences. Its infrastructural backbone consists in an open source archival platform (Dataverse) nourished from the capture of public parts of newspaper and social media, published in all Europe and in all languages since December 2019, including the term “Covid” in their title. \nThe interface is designed as a searchable timeline, where captures are displayed as thumbnails, and sources as well as topics are color coded. The captures are indexed using AI techniques and links are provided to the original contents. The interactive interface is accompanied by a number of static overview maps, tracing bursts and changes in terminology, paying attention to geographic spread and multilinguality. \nThe general design of the Now.museum as a space of cultural heritage, is applicable for any memory-collecting and sharing activity among the wider public, hence individual contributions are expected. To this end, a workflow is designed and a first implementation of ‘citizen/crowd material’ is given.\n\nPS: Domain names Now.museum and Covid-19.museum are the ownership of Yves Rozenholc since the beginning of April 2020. Their denominations have been validated by the ICOM (International Council of Museums) which controls the extension museum.\n",
      "makers": [
        "andrea-scharnhorst/readme",
        "vyacheslav-tykhonov/readme",
        "juliette-vion-dury/readme",
        "yves-rozenholc/readme"
      ],
      "references": [
        "A github repository  https://github.com/C19M/Covid-19.Museum describes the case, and contains links to the software code used. The timeline in essence runs on a Dataverse instance.\n\nReferences around the Covid-19 Museum\nRozenholc Y., Covid-19 Museum ou comment agréger les traces laissées par la pandémie dans un musée virtuel, The Conversation (31 Janvier 2021).\nhttps://theconversation.com/covid-19-museum-ou-comment-agreger-les-traces-laissees-par-la-pandemie-dans-un-musee-virtuel-147327\n\nRozenholc Y. and Tykhonov V., The Covid-19 Museum ... a use case for the Dataverse community, 8th annual Dataverse Community Meeting, Harvard, 14 to 16 June 2022.\nhttps://www.youtube.com/watch?v=3ek7F_Dxcjk&t=3284s \n\nRozenholc Y., Vion-Dury J., Petrazoller F., The Covid-19 Museum, Symposium : Collecte de la rupture ou rupture de la collecte, Science&You Conference, Metz, 16 au 19 Novembre 2021. \nhttps://scienceandyou2021.insight-outside.fr/fr/programme-officiel/37\n\nRozenholc Y., Le musée virtuel du Covid-19, Le patrimoine au temps du confinement, Journée d’études de l’Institut National du Patrimoine 29 Janvier 2021.\nhttps://www.inp.fr/content/download/10713/158132/version/1/file/PROGRAMME.pdf\n\nRozenholc Y., Rencontre-débat « Une pandémie ne change rien », Festival des idées, Université Paris Cité 20 Novembre 2020. https://u-paris.fr/festival-idees-paris/une-pandemie-ne-change-rien/\n\nCovid-19 Museum seminar organized by Rozenholc Y., Vion-Dury J., Covid-19 Museum - YouTube, https://www.youtube.com/@covid-19museum58\n\nOther Media: The Covid-19 Museum, begun in March 2020 and released on April 2, 2020, was featured in the following press:\n\n- 20 Minutes, April 25, 2020\n- Le Point, May 3, 2020\n- La Croix, May 20, 2020\n- The Huffpost, May 21, 2020\n\n- Le Journal Du Dimanche, May 24, 2020 \n- We Demain, August 13, 2020\nhttps://www.wedemain.fr/partager/les-services-darchives-memorisent-nos-vies-confinees\n- Le Monde, May 16, 2021 \n\nand the topic of interview by: \n- France 24 for CulturePrime FRANCE 24 - Le Covid-19 au musée | Facebook\n- Whatsupdoc-lemag.fr N°5 https://www.whatsupdoc-lemag.fr/magazine/51\n"
      ]
    },
    "es": {
      "makers": [
        "andrea-scharnhorst/readme",
        "vyacheslav-tykhonov/readme",
        "juliette-vion-dury/readme",
        "yves-rozenholc/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 7,
      "year": 2023,
      "title": "Mapping the New Politics of Care",
      "body": "Authors: Laura Kurgan, Gregg Gonsalves, Dare Brawley, Jia Zhang, Suzan Iloglu,Thomas Thornhill, Adeline Chum, \n\nLink to Macroscope: <https://newpoliticsofcare.net/>\n\nAbstract: \n\nMapping the New Politics of Care displayed the effects of COVID-19 in the United States layering a wide range of social, economic, and environmental conditions on two interactive maps utilizing multiple vulnerability indexes. \n\nThe first map, titled Community Health Workers, asked viewers to imagine what would happen if one million community workers were allocated across the USA by comparing four indexes of vulnerability alongside COVID-19 data. A user can toggle between multiple vulnerabilities for the distribution of the workers, specific to each county. Making decisions like this responsibly requires confronting and addressing not just the virus but also the inequalities and vulnerabilities that propelled the pandemic.\n\nAll people in the USA were granted the right to have access to free, safe, and effective COVID-19 vaccines over multiple federal distribution phases. But due to the limited doses available, the majority of states followed the Center for Disease Control’s recommendations to prioritize health-care personnel and long-term care facility residents in receiving the first vaccines. \n\nThe second map titled, Vaccine Allocation, focused on this first distribution, Phase 1A of the federal program and compared how each state could have distributed 22 million doses of vaccine more equitably. Users can compare Phase 1A allocation vs allocation to vulnerable populations county by county, in each state. Prioritizing vulnerability could have mitigated health inequities and protected communities that were more likely to have the first and most concentrated COVID-19 outbreaks.\n\nThe vulnerabilities made visible in both maps predated the pandemic and created the conditions for the virus to flourish. Both maps display the acute inequalities embedded in the social and political landscape of the USA. This pandemic is not simply biological. It is a symptom of an illness in our body politic. As COVID-19 roared across the country, it followed the fault lines of social vulnerability.\n",
      "makers": [
        "laura-kurgan/readme",
        "gregg-gonsalves/readme",
        "dare-brawley/readme",
        "jia-zhang/readme",
        "suzan-iloglu/readme",
        "thomas-thornhill/readme",
        "adeline-chum/readme"
      ],
      "references": [
        "Gonsalves, Gregg, and Amy Kapczynski. 2020. “The New Politics of Care.” Boston Review, April 26, 2020. http://bostonreview.net/politics/gregg-gonsalves-amy-kapczynski-new-politics-care.\n\nCenters for Disease Control and Prevention, “COVID-19 Vaccine Distribution Allocations by Jurisdiction - Pfizer,” 2020, https://data.cdc.gov/Vaccinations/COVID-19-Vaccine-Distribution-Allocations-by-Juris/saz5-9hgg.\n\nCenters for Disease Control and Prevention, “COVID-19 Vaccine Distribution Allocations by Jurisdiction - Moderna,” 2020, https://data.cdc.gov/Vaccinations/COVID-19-Vaccine-Distribution-Allocations-by-Juris/b7pe-5nws.\n\nCenters for Disease Control and Prevention. 2020. “COVID-19 Hospitalization and Death by Age.” February 11, 2020. https://www.cdc.gov/coronavirus/2019-ncov/COVID-data/investigations-discovery/hospitalization-death-by-age.html.\n\nCenters for Disease Control and Prevention, “CDC Social Vulnerability Index (SVI),” September 4, 2020, https://www.atsdr.cdc.gov/placeandhealth/svi/index.html.\n\nCenters for Disease Control and Prevention. 2020. “Coronavirus Disease 2019 (COVID-19) 2020 Interim Case Definition, Approved August 5, 2020.” https://wwwn.cdc.gov/nndss/conditions/coronavirus-disease-2019-COVID-19/case-definition/2020/08/05/.\n\nCenters for Medicare and Medicaid Services, “Nursing homes including rehab services, Provider Information,” 2020, https://data.cms.gov/provider-data/dataset/4pq5-n9py.\n\nCenters for Medicare and Medicaid Services. 2020. “State Medicaid and CHIP Applications, Eligibility Determinations, and Enrollment Data, Centers for Medicare and Medicaid Services.” https://data.medicaid.gov/Enrollment/State-Medicaid-and-CHIP-Applications-Eligibility-D/n5ce-jxme.\n\nDepartamento de Salud. 2019. “Informe Anual de Estadísticas Vitales: Defunciones, años 2015 y 2016.” http://www.salud.gov.pr/Estadisticas-Registros-y-Publicaciones/Estadisticas%20Vitales/Informe%20Anual%20Estad%C3%ADsticas%20Vitales%20Defunciones%202015-2016.pdf.\n\nNew York Times. 2020. “Coronavirus (COVID-19) Data in the United States.” https://github.com/nytimes/COVID-19-data.\n\nRemington, Patrick L., Bridget B. Catlin, and Keith P. Gennuso. 2015. “The County Health Rankings: Rationale and Methods.” Population Health Metrics 13 (1): 11. https://doi.org/10.1186/s12963-015-0044-2.\n\nUniversity of Wisconsin Population Health Institute and Robert Wood Johnson Foundation. 2020. “2020 County Health Rankings National Data.” https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation.\n\nUS Bureau of Labor Statistics. 2020. “Local Area Unemployment Statistics.” https://www.bls.gov/lau/laufaq.htm#Q01.\n\nUS Census Bureau, “American Community Survey 5-Year Data (2015–2019),” 2020, https://www.census.gov/data/developers/data-sets/acs-5year.html.\n\nUS Census Bureau, “2018 County Business Patterns,” 2020, https://www.census.gov/programs-surveys/cbp.html.\n\nUS Census Bureau. 2020. “Puerto Rico Municipios Population Totals: 2010–2019.” https://www.census.gov/data/tables/time-series/demo/popest/2010s-total-puerto-rico-municipios.html.\n\nUS Census Bureau. 2019. “American Community Survey 5-Year Data (2014–2018).” https://www.census.gov/data/developers/data-sets/acs-5year.html.\n\nUS Bureau of Labor Statistics. 2020. “Labor Force Data by County, Not Seasonally Adjusted, Latest 14 Months.” https://www.bls.gov/web/metro/laucntycur14.txt."
      ],
      "image": {
        "sm": "mappingthenewpoliticsofcare.png",
        "med": "mappingthenewpoliticsofcare.png",
        "lg": "mappingthenewpoliticsofcare.png"
      }
    },
    "es": {
      "makers": [
        "laura-kurgan/readme",
        "gregg-gonsalves/readme",
        "dare-brawley/readme",
        "jia-zhang/readme",
        "suzan-iloglu/readme",
        "thomas-thornhill/readme",
        "adeline-chum/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 19,
      "sequence": 8,
      "year": 2023,
      "title": "Is Food Making You Sick?",
      "body": "Authors: Yoana Kosturska\n\nLink to Macroscope: <https://thesis.yoanacodes.com/>\n\nAbstract: Is Food Making You Sick? is exploring the relationship between diet and health in 184 countries through the analysis and visualization of extensive data on nutritional factors and diseases. The project aims to study and visualize these relationships by juxtaposing 47 nutrients and 84 health conditions, allowing users to explore and analyze almost four thousand unique relationships. By exploring the patterns between food consumption and health outcomes, users could better inform their dietary choices and be empowered by high quality, research data. Exposing the data visually could also spark discussion amongst researchers, setting the stage for testing hypotheses and performing additional studies, challenging the knowledge we currently have. \n\nThe tool utilizes over 6 GB of data from the Global Dietary Database and the Global Burden of Disease. The data were cleaned, synchronized, and meaningful relationships were selected to be displayed in the final work. Visualizing these correlations in a highly customizable, interactive scatterplot, empowers users to explore relationships, analyze, and interpret complex data in a single, multilayered visualization. The powerful filtering options allow users to increase or decrease the complexity by adding or removing layers of information, utilizing color-coding or sizing of the circles representing each country to convey their geographic location or display additional metrics, such as population, obesity or underweight rates. This empowers users to customize their experience to reflect their analytical needs. Analysis is further supported by the ability to plot a regression line, indicating the direction and strength of each relationship. \n\nVisualizing close to 4,000 unique relationships, the tool allows users to explore and analyze large amounts of data in a highly analytical way - an important starting point in learning more about what are the ways in which the food we consume may influence our health. \n\nThis interactive article presents a so-called “explorable explainer” of the k-means clustering algorithm. It attempts to push the envelope of how visual explanations can be designed on an interactive computational medium such as the web.\n\nThis article explains the algorithm using a “scrollytelling” piece. As the reader scrolls through the article, not only do the visuals animate and transition accordingly with the text, the reader is also prompted to interact with these geometric visualizations of the algorithm, supplementing their intuition through hands-on experimentation with its inner workings.\n\nAs the article progresses and introduces more concepts, it also slowly introduces corresponding knobs and dials that the reader can play around with. The article finally concludes with a dashboard or sandbox that comprises all the interactive controls that came before, giving the reader free rein to probe around and dissect the algorithm to improve their own understanding of its details as they please.\n",
      "makers": [
        "yoana-kosturska/readme"
      ],
      "references": [
        "Global Burden of Disease (GBD): https://www.healthdata.org/gbd/2019\n\nGlobal Dietary Database (GDD): https://www.globaldietarydatabase.org/\n"
      ],
      "image": {
        "sm": "isfoodmakingyousick.png",
        "med": "isfoodmakingyousick.png",
        "lg": "isfoodmakingyousick.png"
      }
    },
    "es": {
      "makers": [
        "yoana-kosturska/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 1,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://traces.pmcruz.com/>\n\nThis project celebrates multi-racial families in the United States from 1860 to 2020, highlighting the gradual dissolution of systemic barriers against racial intermingling in households. Historically, homogenous communities were prevalent, with multi-racial families being rare. Census data before 1960 showed only traces of such families, as race was categorized by enumerators. Post-1970, household members reported their race, marking a shift in racial identification. \n\nKey legal milestones include the 1967 Loving v. Virginia ruling, which lifted restrictions on multi-racial marriages, and the 2015 Obergefell v. Hodges decision, legalizing same-sex marriages. It wasn't until after 2000 that individuals could officially identify as multi-racial. \n\nThe visualization presents each multi-racial couple as a colorful chromosome, detailing the races, ages, sexes, and children of each family. Couples are organized by rarity, age, and number of children, highlighting the evolving structure of American families. Same-sex couples are included in recent years' data, along with a special notation for Latino/as. \n\nThe tool uses two canvas elements for visualization: one displays all families for a selected year, and the other offers a zoomed-in view based on mouse position. An efficient hashing system links mouse coordinates to family data, reducing computational load. Chromosomes are artistically rendered using p5.js with Rom-Catmull splines, and motion is simulated through Perlin noise function. Ages are represented by varying heights, using a square root scale for better visual consistency. \n\nData for this project comes from the anonymized micro-census of the IPUMS USA database, processed through MongoDB to parse households, identify multi-racial families, and structure them as hierarchical JSON objects. This innovative approach not only provides a vivid representation of America's racial diversity but also serves as a testament to the country's evolving societal norms and legal frameworks concerning race and relationships.\n<!--EndFragment-->",
      "externalLink": "https://traces.pmcruz.com/",
      "references": [
        "https://visap.net/2022/contributions/diversity-traces"
      ],
      "makers": [
        "pedro/readme"
      ],
      "title": "Diversity Traces",
      "image": {
        "lg": "image1-opt-1920.webp",
        "med": "mediumsmall1.png",
        "sm": "mediumsmall1.png"
      },
      "year": 2020,
      "iteration": 20
    },
    "es": {
      "makers": [
        "pedro/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 20,
      "sequence": 10,
      "year": 2020,
      "title": "Gaia Stellar Family Portrait",
      "body": "<!--StartFragment-->\n\nMacroscope link: <https://sci.esa.int/gaia-stellar-family-portrait/>\n\nThe ESA Gaia’s Stellar Family Portrait visualisation is an exploration of the Hertzsprung–Russell diagram, a fundamental tool in astronomy, using data from the second data release of ESA’s Gaia mission. The second Gaia data release was made public on 25 April 2018. Learn more about Gaia: sci.esa.int/gaia. \n\nThe Gaia data is the product of a vast human collaboration: the Gaia Data Processing and Analysis Consortium (DPAC), a large pan-European team of about 450 scientists and software developers who are entrusted with the complex task of analysing and processing the satellite data to produce the Gaia catalogues. \n\nThe ESA Gaia’s Stellar Family Portrait visualisation was developed for the European Space Agency by Jan Willem Tulp (TULP interactive) with support from Jos de Bruijne (ESA), Claudia Mignone (ATG Europe for ESA) and Tineke Roegiers (HE Space for ESA). Acknowledgement: Karen O’Flaherty (EJR-Quartz for ESA).\n\n<!--EndFragment-->",
      "makers": [
        "jan-willem-tulp/readme"
      ],
      "externalLink": "https://sci.esa.int/gaia-stellar-family-portrait/",
      "image": {
        "lg": "gaialg.png",
        "sm": "gaiasm.png",
        "med": "gaiasm.png"
      }
    },
    "es": {
      "makers": [
        "jan-willem-tulp/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 11,
      "body": "<!--StartFragment-->\nMacroscope Link: <https://lindseypoulter.com/wdvp/>\n\nThis interactive dashboard encourages users to focus on one area – a country, region, or income level – and see how it compares to its counterparts across a wide range of metrics. It provides multiple ways to interact and explore – the user can search for a country of interest, use the shuffle button to randomly select a focus, or change the selection via tooltips. For best experience, view on a large screen. Recognition: Winner of the World Data Viz Prize\n\n<!--EndFragment-->",
      "externalLink": "https://lindseypoulter.com/wdvp/",
      "references": [
        "<!--StartFragment-->\n\nThe main dataset and metric selection were sourced from [WDVP](https://docs.google.com/spreadsheets/d/1_xdns_UCtRNH9TWcxKYKa_HydlkZxbqCCYRfdxhUNpg/edit#gid=0). The right-hand side of this visualization details the original source of the data. Region and income level classifications originate from the World Bank, via [Gapminder](https://docs.google.com/spreadsheets/d/1qHalit8sXC0R8oVXibc2wa2gY7bkwGzOybEMTWp-08o/edit#gid=501532268).\n\n<!--EndFragment-->"
      ],
      "makers": [
        "lindsey-poulter/readme"
      ],
      "title": "How Do We Compare?",
      "image": {
        "lg": "dashboard.png",
        "sm": "comparesm.png",
        "med": "comparesm.png"
      },
      "year": 2022,
      "iteration": 20
    },
    "es": {
      "makers": [
        "lindsey-poulter/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 20,
      "sequence": 12,
      "year": 2022,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://www.worldwateratlas.org/>\n\nA platform for action, sharing all your compelling narratives on water\n\n<!--EndFragment-->",
      "title": "World Water Atlas",
      "makers": [
        "world-water-atlas/readme"
      ],
      "externalLink": "https://www.worldwateratlas.org/",
      "image": {
        "sm": "watersm.png",
        "med": "watersm.png",
        "lg": "worldwater.png"
      }
    },
    "es": {
      "makers": [
        "world-water-atlas/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 13,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://ourworldindata.org/human-development-index>\n\nMeasuring human development helps us understand how people’s lives and livelihoods vary across the world and how they have changed over time. \n\nThere are several prominent measures that try to capture these changes: \n\n* The Human Development Index (HDI) \n* The Inequality-adjusted Human Development Index (IHDI) \n* The Gender Development Index (GDI) \n* The Augmented Human Development Index (AHDI) \n\nThe first three are published by the United Nations Development Programme. The AHDI, meanwhile, was developed by the economic historian Leandro Prados de la Escosura. \n\nAll these measures seek to broaden the scope of development beyond simple economic growth and to capture other key metrics that track peoples’ living standards. \n\nHowever, measuring human development comes with many challenges. People do not always agree on what should be included. And even once defined, features of human development are difficult to measure. So how do these indices track human development? And what can we learn from them? We summarize the similarities and differences between the different approaches in this article and how to decide on which one to use.\n\n<!--EndFragment-->",
      "externalLink": "https://ourworldindata.org/human-development-index",
      "references": [
        "<!--StartFragment-->\n\nBastian Herre and Pablo Arriagada (2023) - “The Human Development Index and related indices: what they are and what we can learn from them” Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/human-development-index' \\[Online Resource]\n\n<!--EndFragment-->"
      ],
      "makers": [
        "bastian-herre/readme",
        "pablo-arrigada/readme"
      ],
      "title": "The Human Development Index and related indices: what they are and what we can learn from them",
      "image": {
        "lg": "humandevelopmentlg.png",
        "sm": "humandevelopmentsm.png",
        "med": "humandevelopmentsm.png"
      },
      "year": 2023,
      "iteration": 20
    },
    "es": {
      "makers": [
        "bastian-herre/readme",
        "pablo-arrigada/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 14,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <http://higlass.io/>\n\nHiGlass is a tool for exploring and comparing genomic contact matrices and tracks. Take a look at some examples or head over to the docs to learn how HiGlass can be used and configured. To load private data, HiGlass can be run locally within a docker container.\n\n<!--EndFragment-->",
      "externalLink": "http://higlass.io/",
      "references": [
        "<!--StartFragment-->\n\n\\[1]Kerpedjiev et al.HiGlass: Web-based visual comparison and exploration of genome interaction maps.Genome Biology, 19:125 (2018).\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\n\\[2]Rao et al.A 3D map of the human genome at kilobase resolution reveals principles of chromatin looping.Cell 159.7 (2014): 1665-1680.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\n\\[3]Busslinger et al.Cohesin is positioned in mammalian genomes by transcription, CTCF and Wapl.Nature 544.7651 (2017): 503-507.\n\n<!--EndFragment-->"
      ],
      "makers": [
        "nils-gehlenborg/readme"
      ],
      "title": "HiGlass",
      "image": {
        "lg": "higlasslg.png",
        "sm": "higlasssm.png",
        "med": "higlasssm.png"
      },
      "year": 2018,
      "iteration": 20
    },
    "es": {},
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 20,
      "sequence": 15,
      "year": 2023,
      "title": "Artificial intelligence has advanced despite having few resources dedicated to its development – now investments have increased substantially",
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://ourworldindata.org/ai-investments>\n\nArtificial intelligence (AI) technology has steadily become more powerful over the course of the last decades and in recent years it has entered our world in many different domains. In a companion article – the brief history of artificial intelligence – I document this development. \n\nThis was achieved despite having relatively few resources. Until recently, investments in terms of capital and scientific efforts were small. In this article I highlight that this has very much changed in recent years. Corporate investment has increased and the scientific field has grown in size. Given how rapidly AI developed in the past, despite the limited resources, this should make us expect AI technology to continue to become more powerful in the coming decades.\n\n<!--EndFragment-->",
      "makers": [
        "max-roser/readme"
      ],
      "externalLink": "https://ourworldindata.org/ai-investments",
      "image": {
        "lg": "ailg.png",
        "sm": "aism.png",
        "med": "aism.png"
      }
    },
    "es": {},
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 20,
      "sequence": 16,
      "year": 2020,
      "title": "Research Funding Landscape",
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://www.cwts.nl/rori/fundinglandscape/?map=global>\n\nThis funding landscape shows 2890 research fields across all sciences. By changing the size and color coding, the contribution of a specific research funder to the different fields can be made visible.\n\n<!--EndFragment-->",
      "externalLink": "https://www.cwts.nl/rori/fundinglandscape/?map=global",
      "makers": [
        "nikita-rokotyan/readme"
      ],
      "image": {
        "lg": "researchlg.png",
        "sm": "researchsm.png",
        "med": "researchsm.png"
      }
    },
    "es": {},
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 20,
      "sequence": 17,
      "year": 2023,
      "title": "Sustainability Cosmos",
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://palminister.github.io/sustainability-cosmos/>\n\nWelcome to the cosmic realm of sustainability, where the stars align to reveal the true nature of our world's progress. In this interactive dashboard, we invite you to embark on a journey through the galaxy of data, where each star represents a country and its position in the constellation reflects multiple things depending on the dashboard views. As you navigate through the vast expanse of information, you'll discover the secrets of each country's performance in terms of Human, Health, Environmental, Economical, and Political conditions—all that makeup “Sustainability”. Furthermore, this journey will also reveal opportunities for improvement and growth toward a more sustainable future. Join us as we uncover the mysteries of the Sustainability Cosmos, and gain a deeper understanding of the current state of our world.\n\n<!--EndFragment-->",
      "externalLink": "https://palminister.github.io/sustainability-cosmos/",
      "makers": [
        "palm-jumnongrat/readme",
        "supawich-orian/readme"
      ],
      "image": {
        "lg": "cosmoslg.png",
        "sm": "cosmossm.png",
        "med": "cosmossm.png"
      }
    },
    "es": {
      "makers": [
        "palm-jumnongrat/readme",
        "supawich-orian/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 18,
      "body": "Macroscope Link: <https://remo.cua.uam.mx/vis/Exploratorium/>\n\nThe Space-Time Theories Exploratorium's Macroscope is a cutting-edge tool designed to cater to a broad spectrum of users, serving the needs of both students and researchers in Physics, Philosophy of Science, Foundations of Physics, Ontology, Structuralism, Complexity, and Graphic Representations. This web-based interactive platform utilizes HTML with D3 JavaScript and CSS for visual explorations.\n\nThe Macroscope enables users to delve into ten interactive visualizations, encompassing over 50 theories of space-time and gravitation. It facilitates the location of groups of theories, their foundations, and classes of models. Through hierarchical networks, users learn the logic of interconnected concepts, becoming adept at navigating the conceptual structure of physical theories. \n\nData is sourced from axiomatizations and reconstructions of physical theories conducted by renowned philosophers of science and physicists such as Michael Friedman, Mario Bunge and Clifford Will. This comprehensive dataset, efficiently managed in a SQL datafile, forms the backbone of the Macroscope's functionality. \n\nThe tool empowers users to perform in-depth data analysis, comparing subtractive and additive reconstructions, uncovering relationships of theorization and specialization. With interactive features like zoom-in, zoom-out, and drag capabilities, users can dynamically explore, rearrange, and personalize their network settings, enhancing the overall user experience. \n\nThe Macroscope allows users to highlight subconcepts and supraconcepts, offering a nuanced perspective. It also facilitates the highlighting of specific nodes, enabling users to pinpoint particularly interesting elements within the networks. This tool is not just a visualization aid; it serves as a companion for source readings, enhancing the understanding and contextualization of academic materials. \n\nIn summary, the Space-Time Theories Exploratorium's Macroscope is a versatile and user-friendly tool that bridges interdisciplinary gaps, providing a seamless exploration of the complex relationships within space-time and gravitation theories. Whether for educational purposes or advanced research, this Macroscope offers a rich and interactive experience for users across various domains of interest.",
      "externalLink": "https://remo.cua.uam.mx/vis/Exploratorium/",
      "references": [
        "<!--StartFragment-->\n\nEspinosa, M. “The Space-Time Theories Exploratorium” in Scientific Legacy of Professor Zbigniew Oziewicz. Ed. Colín García, Hilda María and Guzmán, José de Jesús Cruz and Kauffman, Louis H and Makaruk, Hanna, WORLD SCIENTIFIC, 2023 pp. 225-245. 10.1142/13275}, www.worldscientific.com/doi/abs/10.1142/13275}.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nEspinosa, M. & Casanueva, M. “Retículos teóricos y el análisis de conceptos formales, herramientas para el estructuralismo metateórico”, Theoria: An International Journal for Theory, History and Foundations of Science, DOI: 10.1387/theoria.24078, March, 2023.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nEspinosa, M. ”Retículos conceptuales de teorías físicas: El Exploratorium\" in Reflexiones filosóficas e históricas: ciencia, enseñanza de la ciencia y política científica. Lilian Al-Chueyr Pereira Martins, Luz Marina Duque Martínez, Lucía Federico, Germán Guerrero Pino, María de las Mercedes O’Lery (Eds.), Universidad del Valle, Colombia, 2023. ISBN: 978-65-86622-03-4\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nEspinosa, M. Visualizaciones de retículos conceptuales de teorías del espacio-tiempo y la gravitación mediante el análisis de conceptos formales. PhD Thesis, Universidad Autónoma Metropolitana-Cuajimalpa, March 2022.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nEspinosa, M. “Mapping Gravity with Hierarchical Networks”, Proceedings: 15th Marcel Grossmann Meeting on Recent Developments in Theoretical and Experimental General Relativity, Astrophysics, and Relativistic Field Theories (MG15, 2018).\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nEspinosa, M. “The Gravity Apple Tree”, Journal of Physics: Conference Series, Vol. 600, No 1, 2014.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\n<https://github.com/pupitetris/Exploratorium>\n\n<!--EndFragment-->"
      ],
      "makers": [
        "mariana-espinosa/readme"
      ],
      "title": "Space-Time Theories Exploratorium",
      "image": {
        "sm": "physicssm.png",
        "med": "physicssm.png",
        "lg": "physicslg.png"
      },
      "year": 2023,
      "iteration": 20
    },
    "es": {},
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 2,
      "body": "<!--StartFragment-->\nMacroscope Link: <https://river-runner-global.samlearner.com/>\n\nThe river runner tool helps people to better understand the interconnectedness of our watersheds by calculating the downstream path from anywhere on earth and visualizing the journey. \n\nIn its original iteration, it relied on the USGS's NLDI, which links water features and flowpaths within the United States. The dataset compiled for the global version created in collaboration with hydrologists at the USGS and the Internet of Water, though it remains a work in progress (particularly with regards to its coverage of water feature names). \n\nThe frontend for the visualization was built with svelte and mapbox, putting the user in control of a 3D journey from a place of their choosing to an ocean or inland lake. The aim was to zoom in with an immersive flyover experience to give a sense of how many places and communities are downstream of one another. I also wanted to provide navigation widgets in the corners to orient someone in the full network of streams, tributaries, and rivers it takes to reach an ocean or endhoric basin.\n\n<!--EndFragment-->",
      "externalLink": "https://river-runner-global.samlearner.com/",
      "references": [
        "<!--StartFragment-->\n\nThere's a list of sources for the data/attributions here: https://ksonda.github.io/global-river-runner/ (Also a note that Dave Blodgett (USGS), Kyle Onda (Internet of Water) and Ben Webb (Internet of Water) should be credited somewhere for their work on the backend here, though I don't know if they'd like me to fill out full contact details for them on this form)\n\n<!--EndFragment-->"
      ],
      "makers": [
        "sam/readme"
      ],
      "title": "River Runner",
      "image": {
        "lg": "riverrunnerlg.png",
        "med": "riverrunnersm.png",
        "sm": "riverrunnersm.png"
      },
      "year": 2020,
      "iteration": 20
    },
    "es": {
      "makers": [
        "sam/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 3,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <http://www.go4trees.com/four-seasons/>\n\nThis interactive dashboard, created for the World Government Summit, aims to visualize the world's development indicators. The visualization presents different seasons, emphasizing various aspects. \n\nSpring is a time of growth and development. \n\nSummer is a time of prosperity, yet a few countries face economic hardship. Economic prosperity can also be seen as a sign of success and abundance. However, only some have equal access to these resources, and inequalities can arise when some people have more access to wealth, opportunities, and resources than others. \n\nFall is a time of change, as the leaves change color and fall to the ground. It is also a sign of the bounty for fruits.\n\nWinter is a time of preparation for crisis. Addressing the energy and climate crisis may require a collective effort to prepare for the challenges ahead. Snow here is a symbol of the challenges and difficulties that we face as a society. \n\nIn each season, it presents a tree that tells a performance story by visualizing the performance metrics of each sub-region and country. The tree roots present the global performances over the past ten years. The past provides the foundation for the present and future, just like the roots of a tree. \n\nThe sunlight and rain in the sky visualize the buzz of 20 development and 18 innovation frontiers from seven categories. Just as the future holds the potential for growth and development, development and innovation are like the sunlight and rain that provide energy and water for a tree to grow and flourish.\n\n<!--EndFragment-->",
      "externalLink": "http://www.go4trees.com/four-seasons/",
      "references": [
        "https://informationisbeautiful.net/2023/the-winners-of-the-world-dataviz-prize-2023/"
      ],
      "makers": [
        "liuhuaying-yang/readme"
      ],
      "title": "FOUR SEASONS: How Has the World Progressed in Recent Years?",
      "image": {
        "lg": "treeslg.png",
        "med": "treessm.png",
        "sm": "treessm.png"
      },
      "year": 2022,
      "iteration": 20
    },
    "es": {
      "makers": [
        "liuhuaying-yang/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 4,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://vis.csh.ac.at/global-supply-chain-maps/>\n\nThe Russia-Ukraine conflict has highlighted the intricate nature of global supply chains, a complex network connecting nations and propagating disruptions to distant regions. Previous studies often concentrate on direct dependencies, neglecting indirect dependencies caused by the lack of essential inputs, making a comprehensive assessment of the global supply system challenging. \n\nIn response to these complexities, we have developed a visualization based on research at CSH that allows to explore different scenarios and provides a holistic view of both indirect and direct global effects of supply disruptions. \n\nThe tool delves into two critical aspects: food availability and trade sanctions. It demonstrates how localized production disruptions can have far-reaching implications, impacting trade relationships and the entire production chain. For instance, a shock to Ukrainian maize production not only affects maize availability but also leads to losses in pig or poultry meat due to a shortage of animal feed. This aspect of the visualization helps us fully appreciate the extent of potential losses when a specific product ceases production in a country. \n\nAdditionally, the project explores the cascading effects of import scarcity caused by trade sanctions. As primary and intermediary imports decline, domestic production is disrupted, and demand for goods in the affected country decreases. This creates a ripple effect, influencing global trade dynamics. \n\nOur visualization tool is based on robust research models, and it is not only informative but also interactive, allowing users to explore the consequences of different scenarios involving global supply disruptions in a dynamic and engaging manner. By going beyond traditional representations, it engages viewers and connects them to the underlying data. \n\nThrough this project, we hope to foster a deeper understanding of the intricacies of supply chains and their vulnerabilities. As the world continues to face unpredictable challenges, this visualization stands as a powerful resource for gaining insights into the dynamics of the global economy and potential vulnerabilities of supply networks for policymakers and businesses alike.\n\n<!--EndFragment-->",
      "externalLink": "https://vis.csh.ac.at/global-supply-chain-maps/",
      "references": [
        "<!--StartFragment-->\n\nhttps://doi.org/10.1038/s43016-023-00771-4\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nhttps://www.csh.ac.at/economic-shocks-shaking-russia-and-the-world/\n\n<!--EndFragment-->"
      ],
      "makers": [
        "liuhuaying-yang/readme"
      ],
      "title": "The Whole Picture: Cartographic Insights into Global Supply Chain Dependency",
      "image": {
        "lg": "supplychainlg.png",
        "med": "supplychainsm.png",
        "sm": "supplychainsm.png"
      },
      "year": 2022,
      "iteration": 20
    },
    "es": {
      "makers": [
        "liuhuaying-yang/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 5,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://www.theshapeofchange.com/>\n\nCan we estimate how much the world has changed in a decade? Or do our own experiences impact the perception of progress? The work, titled The Shape of Change, challenges our assumptions about how key statistical indicators regarding Health, the Environment, or Education evolve through the years. The work was done in the context of the World Data Visualization Prize, part of the World Government Summit 2023. For that year, the competition focused on the past, present, and future of society and governments. The goal was to create a tool to showcase how data visualization can show data on innovations, decisions, and metrics that can be used to drive and measure progress. The organization made available three datasets, from which participants should create the visualizations. The dataset used in this work contained yearly observations for statistical indicators between 2012 and 2022. The work is a scrolly-telling piece that starts by taking the reader through significant news stories of the past decade. The goal is to connect the reader to the indicators by remembering specific events. Then, they are prompted to guess how an indicator related to the event changed, challenging the perception of the immediacy of news stories with the continuous change during the decade. Additionally, it has been shown \\[1] that users prompted to reflect on their prior knowledge by predicting and self-explaining improve their recall and comprehension of the data. These exercises also serve to introduce the user to the visual element used to represent change: the angle, direction, and color of the triangles. This knowledge is then needed to interpret the small multiples visualization at the end of the piece, where the reader can have a bird's-eye view of the yearly change for all the indicators.\n\n<!--EndFragment-->",
      "externalLink": "https://www.theshapeofchange.com/",
      "references": [
        "<!--StartFragment-->\n\n\\[1] Explaining the Gap: Visualizing One’s Predictions Improves Recall and Comprehension of Data, https://idl.cs.washington.edu/files/2017-ExplainingTheGap-CHI.pdf\n\n<!--EndFragment-->"
      ],
      "makers": [
        "rita-costa/readme",
        "beatriz-malveiro/readme"
      ],
      "title": "The Shape of Change",
      "image": {
        "lg": "changlg.png",
        "med": "changesm.png",
        "sm": "changesm.png"
      },
      "year": 2020,
      "iteration": 20
    },
    "es": {},
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 6,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://vimeo.com/901381287>\n\nThis project explores the relationship between the capitalist system and its implications for climate change. Rooted in a critical examination of the capitalist pursuit of profit, perpetual consumption patterns, and the resulting inequality between citizens of “developed” and “developing” economies, it highlights the profound impact of these factors on the escalating climate crisis. Driven by H.A Baer’s work which emphasizes the role of capitalist ideology in perpetuating reliance on fossil fuels and contributing to greenhouse gas emissions \\[1], we leverage data from the World Bank on greenhouse gas emissions per capita \\[2], and the Germanwatch Global Climate Risk Index (CRI) \\[3]. The CRI is a weighted index of 4 measures used to evaluate the losses a country has experienced from extreme weather events connected to climate change. The losses analyzed are economic losses (absolute losses of purchasing power parity, and losses per GDP unit), and loss of life (number of lives lost, and deaths per 100,000 inhabitants). \n\nThe visualization was created in processing using particle systems and a physics library to simulate realistic hurricane behavior. Particles leave short trails and have a centrifugal motion: rotating closer to the center than in the outer rim. The hurricanes have different eye sizes, total sizes, and speeds based on data values. For very at-risk countries, you would see a large hurricane with a small eye rotating quickly. The particles extinguish when they reach the eye of the hurricane and are then reseeded in the periphery. Ultimately, we aim to visually represent climate vulnerabilities juxtaposed with global greenhouse gas emissions to shed light on the relationship between risk and emissions at the country-level granularity. We emphasize that for high emitting nations, while pollution has historical been something to bury or ship away, climate risk cannot be exported.\n\n<!--EndFragment-->",
      "externalLink": "https://vimeo.com/901381287",
      "references": [
        "<!--StartFragment-->\n\n\\[1] H.A Baer, “Global Capitalism and Climate Change”, 2012.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\n\\[2] Hannah Ritchie, Pablo Rosado and Max Roser (2023), “CO₂ and Greenhouse Gas Emissions”, Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/co2-and-greenhouse-gas-emissions'.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\n\\[3] David Eckstein, Vera Kunzel, Laura Schafer, for Germanwatch, “Global Climate Risk Index 2021”, and “Global Climate Risk Index 2020”. Retrieved from: https://www.germanwatch.org/en/19777 and https://www.germanwatch.org/en/17307\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nProject website: https://creativeclimatelab.sites.northeastern.edu/visualizing-national-climate-risk-inequities/\n\n<!--EndFragment-->"
      ],
      "makers": [
        "chloe-hudson-prock/readme",
        "pedro/readme",
        "gregory-gold/readme"
      ],
      "title": "A Perfect Storm: Visualizing Climate Risk and Emissions",
      "image": {
        "lg": "climatelg.png",
        "sm": "climatesm.png",
        "med": "climatesm.png"
      },
      "year": 2020,
      "iteration": 20
    },
    "es": {
      "makers": [
        "chloe-hudson-prock/readme",
        "pedro/readme",
        "gregory-gold/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 7,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://genetic-flow.com/>\n\nGeneticFlow, our Macroscope, is intricately designed for the academic user groups at large, e.g., researchers, educators, decision-makers, or even fresh students in universities and research institutions. The tool meets an important need of academic researchers on the in-depth analysis and interpretation of scholars' impact, streamlining tasks such as scientific career analysis, academic award selection, and tenure evaluation. For junior students, it also opens a door for visually understanding the career path and biography of prestigious scientists, e.g., Turing award winners and Nobel prize laureates. Now we focus on several areas inside the computer science discipline, including AI, database, graphics, visualization, etc., but we may extend the coverage to more disciplines in near future (e.g., physics, material). \n\nThe GeneticFlow tool mainly utilizes the comprehensive dataset provided by Microsoft Academic Graph, which encompasses an almost full coverage of academic data including papers, authors, citations, and topics, till 2021.10. At its core, the GeneticFlow tool designed a self-citation graph to visualize the evolution of a scholar’s research innovations, as well as their external impact. Cutting edge algorithms, including advisor-advisee detection, extend-type citation classification, embedding-based topic generation, and graph neural networks are employed in our tool for both effective visualization and downstream inference tasks. More details can be found in our KDD 2023, VIS 2023, and NeuroIPS 2023 papers. \n\nThe visualization design employs a time-dependent hierarchical graph representation, further augmented with appropriate color mapping, interactive trend charts, and a topical map view, providing multidimensional insights into temporal and thematic aspects of a scholar's research contribution and impact. Through extensive case studies and quantitative evaluations, we find that the GeneticFlow visualization can highlight the key success of top-ranked scholars. It also predicts high-level award recipients (e.g., ACM fellows), with significantly better performance than classical impact indicators such as h-index, i-10 index, and citation networks.\n\n<!--EndFragment-->",
      "externalLink": "https://genetic-flow.com/",
      "references": [
        "<!--StartFragment-->\n\nYuankai Luo, Lei Shi\\*, Mufan Xu, Yuwen Ji, Fengli Xiao, Chunming Hu, Zhiguang Shan\\*,“Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs”, KDD, 2023.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nFengli Xiao and Lei Shi*, “GeneticFlow: Exploring Scholar Impact with Interactive Visualization ”, IEEE VIS (short paper), 2023.\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nYuankai Luo, Veronika Thost\\*, and Lei Shi\\*, “Transformers over Directed Acyclic Graphs”, NeurIPS, 2023.\n\n<!--EndFragment-->"
      ],
      "makers": [
        "fengli-xiao/readme",
        "ye-sun/readme",
        "yuankai-luo/readme",
        "lei-xia/readme",
        "weize-wu/readme",
        "chunming-hu/readme",
        "lei-shi/readme"
      ],
      "title": "GeneticFlow: Exploring Scholar Impact with Visualization",
      "image": {
        "lg": "flowlg.png",
        "med": "flowsm.png",
        "sm": "flowsm.png"
      },
      "year": 2023,
      "iteration": 20
    },
    "es": {},
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 8,
      "body": "<!--StartFragment-->\n\nMacroscope Link: https://gamescope.streamlit.app/\n\nGameScope is a macroscope designed for enthusiasts, and researchers interested in exploring the intricate landscape of video game sales spanning from 1996 to 2016. Tailored to meet the diverse needs of the gaming community, this tool empowers users to dive into the vast realm of gaming data, providing insights into sale numbers, genre preferences, and regional best-sellers. \n\nUser Group and Needs Served: GameScope caters to a wide audience, including gamers, researchers, and industry professionals seeking a comprehensive understanding of the video game market. Whether investigating historical sales data or planning future strategies, the tool addresses the nuanced needs of users navigating the dynamic gaming landscape. \n\nData Used: We have used the \"Global Video Game Sales & Ratings\" dataset from Kaggle (https://www.kaggle.com/datasets/thedevastator/global-video-game-sales-ratings). The dataset consists of records from Metacritic providing insight into global video game ratings and sales. It contains data such as publisher, genre, year of release, and sales information. \n\nData Analysis Performed: GameScope performs intricate data analyses based on user-selected parameters. Users can track total game sales, uncover genre-specific and year specific trends, and delve into regional variations to gain a nuanced perspective on the gaming market dynamics during the selected time frame. \n\nVisualization Techniques Applied: GameScope presents users with an interactive map with clickable pop-ups providing information about each region, enhancing the exploration of gaming data. It also makes use of bar and pie chart to display analysed data. \n\nMain Insights Gained: Users of GameScope can extract valuable insights, such as identifying top-selling games, understanding genre preferences over time, and discerning regional variations in game popularity. The tool serves as a compass, guiding users through the intricate world of video game sales and offering key insights.\n\n<!--EndFragment-->",
      "externalLink": "https://gamescope.streamlit.app/",
      "references": [
        "<!--StartFragment-->\n\nhttps://www.kaggle.com/datasets/thedevastator/global-video-game-sales-ratings\n\n<!--EndFragment-->"
      ],
      "makers": [
        "subhadeep-jana/readme"
      ],
      "title": "GameScope",
      "image": {
        "lg": "gamescopelg.png",
        "med": "gamescopesm.png",
        "sm": "gamescopesm.png"
      },
      "year": 2020,
      "iteration": 20
    },
    "es": {
      "makers": [
        "subhadeep-jana/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 9,
      "body": "<!--StartFragment-->\n\nMacroscope Link: <https://marketmap.one/>\n\nMarket Map is an experimental data visualization project that explores new ways of visualizing the US stock market data using dimensionality reduction algorithms. The map turns multidimensional financial data into an interactive three-dimensional exploration of companies publicly traded on the NYSE and NASDAQ.\n\n<!--EndFragment-->",
      "externalLink": "https://marketmap.one/",
      "references": [
        "<!--StartFragment-->\n\nhttps://www.informationisbeautifulawards.com/showcase/6628-market-map\n\n<!--EndFragment-->",
        "<!--StartFragment-->\n\nhttps://interacta.io\n\n<!--EndFragment-->"
      ],
      "makers": [
        "nikita-rokotyan/readme",
        "olya-stukova/readme",
        "sergey-ryadovoy/readme"
      ],
      "title": "Market Map",
      "image": {
        "lg": "marketlg.png",
        "med": "marketsm.png",
        "sm": "marketsm.png"
      },
      "year": 2023,
      "iteration": 20
    },
    "es": {
      "makers": [
        "nikita-rokotyan/readme",
        "olya-stukova/readme",
        "sergey-ryadovoy/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 1,
      "body": "Link to Submitted Work: https://www.kenrinaldo.com/portfolio/opera-for-dying-insects/ \n\nThe Opera of Dying Insects addresses the global tragedy of the insect apocalypse, driven by industrial farming, habitat loss, climate change, deforestation, and pesticide use. Studies reveal a 40% decline in insect populations and a 76% drop in flying insects over 27 years. As insects are vital to pollination and the food chain, this crisis has far-reaching consequences.\nThis sound and video installation features pill bugs (Armadillidium Vulgare) thriving in a moist, constructed environment with domestic insects, fungi, and bacteria. As they consume and decompose a wet log, their movements are tracked by cameras and analyzed with artificial intelligence software. This data triggers an evolving tragic opera they compose. The ecosystem is built from locally sourced soil and insects. A small cube of earth, including branches and pill bugs, are transplanted from a local forest into a glass vitrine. After the exhibition, the soil is returned to its original habitat. \nProjected visuals magnify the pill bugs’ micro-world and insects that are going extinct, encouraging viewers to appreciate their subtle beauty and recognize their critical role in ecosystems. The work aims to inspire a deeper awareness of the fragile interconnectedness of life on Earth.",
      "externalLink": " https://www.kenrinaldo.com/portfolio/opera-for-dying-insects/",
      "references": [
        "https://www.antennae.org.uk/"
      ],
      "makers": [
        "ken-rinaldo/readme"
      ],
      "title": "The Opera of Dying Insects",
      "image": {
        "sm": "operasm.png",
        "med": "operasm.png",
        "lg": "operasm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "ken-rinaldo/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 10,
      "body": "Link to Submitted Work: https://app.powerbi.com/view?r=eyJrIjoiMWExNmQwZTYtNDJiZS00NWJiLWJiOTktN2NhYjRiYmJkNmM4IiwidCI6IjI5ZDRjMTFjLTA1N2MtNDg3Zi04ZmRhLWU4NmQ1OTkzOWU2NCIsImMiOjZ9\n\nThis interactive dashboard features data visualizations of the collection of messages containing \"visual intelligence\" in the X/Twitter platform. The data contains a network of 20,354 X/Twitter users whose recent tweets contained \"visual intelligence\", or who were replied to, mentioned, retweeted or quoted in those tweets, taken from a data set limited to a maximum of 20,000 tweets, tweeted between 1/1/2023 12:00:00 AM and 5/13/2025 9:23:15 PM. The network was obtained from Twitter on Wednesday, 14 May 2025 at 13:03 UTC.\n\nTop influencers: \n@grok\n@sin*ai_1\n@elonmusk\n@visualguide*\n@alekalucard\n@asianjesusamigo\n@agent_mock\n@markgurman\n@videsignsai\n@icreatelife\n\nTop hashtags:\n#ai\n#artificial\n#intelligence\n#apple\n#nftart\n#appleevent\n#nft\n#web3\n#video\n#digitalassets",
      "externalLink": "https://app.powerbi.com/view?r=eyJrIjoiMWExNmQwZTYtNDJiZS00NWJiLWJiOTktN2NhYjRiYmJkNmM4IiwidCI6IjI5ZDRjMTFjLTA1N2MtNDg3Zi04ZmRhLWU4NmQ1OTkzOWU2NCIsImMiOjZ9",
      "references": [
        "http://nodexlgraphgallery.org",
        "http://smrfoundation.org",
        "https://www.pewresearch.org/internet/2014/02/20/mapping-twitter-topic-networks-from-polarized-crowds-to-community-clusters/",
        "Dashboard slides: https://www.dropbox.com/scl/fi/3zyaxtskl52rdkzp1acjm/2025-05-14-NodeXL-X-Twitter-Search-visual-intelligence-1.pptx?rlkey=ktr7quhfpwhrymlfj2hlvon9k&st=hkoikie3&dl=0"
      ],
      "makers": [
        "marc-a-smith/readme-1",
        "arber-ceni/readme"
      ],
      "title": "Network Mapping \"Visual Intelligence\" with the NodeXL Pro INSIGHTS Dashboard",
      "image": {
        "sm": "nmvisualintelsm.png",
        "med": "nmvisualintelsm.png",
        "lg": "nmvisualintelsm.png"
      },
      "year": 2020,
      "iteration": 21
    },
    "es": {
      "makers": [
        "marc-a-smith/readme-1",
        "arber-ceni/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 21,
      "sequence": 11,
      "year": 2025,
      "title": "ecocalypse",
      "body": "Link to Submitted Work: https://www.youtube.com/watch?v=N72qYSQSCSM\n\necocalypse is a post-anthropocene reality, a hypothetical moment of rupture, after which nature, deformed for centuries by human activity, is freed from industrial and cultural constructs. however, this freedom is illusory: not a return to the natural order, but an entry into a hybrid era, where the biological and technogenic, organic and inorganic mutate, giving rise to unexpected symbioses and life forms. a languid voiceover interpreting the ideas of dark ecology speaks about the pulsating topography of changes, in which survival is determined not by the previous laws of nature, but by the ability to adapt to a fluid reality, and the mutating neural network nature that reinforces the speech simultaneously attracts and frightens.",
      "externalLink": "https://www.youtube.com/watch?v=N72qYSQSCSM",
      "makers": [
        "yana-cherkasova/readme"
      ],
      "image": {
        "sm": "ecocalypsesm.png",
        "med": "ecocalypsesm.png",
        "lg": "ecocalypsesm.png"
      }
    },
    "es": {
      "makers": [
        "yana-cherkasova/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 12,
      "body": "Link to Submitted Work: danyagonsales.myportfolio.com/posledovanie \n\nPOSLEDOVANIE is a performative sequence of ritual events in an interactive and sound environment with an emphasis on bio-cybernetic symbolism. It requires active audience participation. It can accompany dance and performances, and be projected onto sculptures and architecture.",
      "externalLink": "danyagonsales.myportfolio.com/posledovanie",
      "references": [
        "A MAZE Berlin 2025 (screening, exhibition)"
      ],
      "makers": [
        "danya-gonsales/readme"
      ],
      "title": "POSLEDOVANIE",
      "image": {
        "sm": "posledovaniesm.png",
        "med": "posledovaniesm.png",
        "lg": "posledovaniesm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "danya-gonsales/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 21,
      "sequence": 13,
      "year": 2025,
      "title": "Insertion Loss",
      "body": "Link to Submitted Work: https://youtu.be/zTcjKiPATbU\n\nInsertion Loss\n\nWords are meaningless and forgettable © Depeche Mode\n\n“Insertion Loss” explores the collaboration between human emotional intelligence and artificial intelligence in the search for new forms of non-verbal communication. The title refers to a term in telecommunications: the weakening of a signal as it passes through a system. SHUM uses this concept as a metaphor for emotional expression — fragile, distorted, often lost in translation between people.\n\nThe work questions the dominance of language as the primary tool for connection. There are emotions we struggle to express. There are words that don’t exist in other languages. There is always something lost.\n\nThis video artwork combines sound design, AI-generated content, and Lev Kuleshov’s montage theory to immerse the viewer in a meditative state — a quiet co-tuning with another presence. Artificial intelligence is not a tool here, but a co-author, helping to reconstruct emotional signals into an audiovisual language.\n\n“Insertion Loss” visualizes a hybrid form of intelligence: one that senses, interprets, and responds. It points to a near future where technologies might read and translate our emotional and physiological states — hormonal changes, facial microexpressions, nervous system responses — and transmit them as shared experiences.\n\nRather than offering a utopia, the work suggests a subtle shift: from explaining to sensing, from interpreting to co-feeling. In a time when language grows louder but not clearer, perhaps cooperation between intelligences — human and artificial — can open new emotional channels.",
      "externalLink": "https://youtu.be/zTcjKiPATbU",
      "makers": [
        "shum/readme"
      ],
      "image": {
        "sm": "insertionlosssm.png",
        "med": "insertionlosssm.png",
        "lg": "insertionlosssm.png"
      }
    },
    "es": {
      "makers": [
        "shum/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 14,
      "body": "Link to Submitted Work: https://www.rinaldimichele.com/latent-xylella/ \n\nLatent Xylella is a triptych of olive trees generated by an artificial intelligence algorithm (GAN – Generative Adversarial Network), trained on over 10,000 photographs personally taken in the fields of Casarano, affected by Xylella fastidiosa, a bacterium that has decimated the centuries-old olive groves of Salento. The work explores AI’s ability to process memory, loss, and the transformation of the landscape.\nDeveloped according to the principles of Green AI, the algorithm shapes a landscape suspended between memory and loss. The images, emerging from the neural network's latent space, evoke an imaginary archive of what once was and perhaps will never be again: ghostly trees, like fading imprints. The triptych, a secular echo of the sacred, pays homage to the olive tree as a symbol of Apulian identity and resistance.\nThe work visualizes the interaction between environmental, algorithmic, and emotional intelligence: human intelligence selects and curates the training images; artificial intelligence processes and transforms them; while emotional intelligence is activated in the viewer’s reception, recognizing in the ethereal forms the echoes of a collective identity at risk.\nInspired by Bosch’s The Garden of Earthly Delights, Latent Xylella responds to the need to understand how artificial intelligences can collaborate with human sensitivity to preserve and re-narrate wounded places. Latent Xylella imagines new roots from what remains, in a land scarred but still capable of telling its story.",
      "externalLink": "https://www.rinaldimichele.com/latent-xylella/",
      "references": [
        "https://www.mdpi.com/2073-445X/13/12/2087"
      ],
      "makers": [
        "michele-rinaldi/readme"
      ],
      "title": "Latent Xylella",
      "image": {
        "sm": "latentxylellasm.png",
        "med": "latentxylellasm.png",
        "lg": "latentxylellasm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "michele-rinaldi/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 15,
      "body": "Link to Submitted Work: https://vimeo.com/619325725\n\nKatherina Sadovsky and Lilia Li-Mi-Yan\nVideo, 3D, CGI, sound\nA000000000001000AA011\n\nThe artists explore the theme of the possibility of human interaction and connection with other forms of existence. What would happen if we had a new body created through interaction with new technologies, materials, bacteria? Will we be eternal, and will we remain the same humans? We are concerned with the question, what will happen to the emotions of the new person, the post-human, the cyborg...? Will we be able to refuse to reproduce ourselves?\n\nThe characters in the video are equipped with special implants and an additional organ system that allows them to survive in the modern world, where many environmental disasters have occurred. Powerful CO2 emissions into the atmosphere have led to global warming, and viruses have destroyed an ordinary biological body, forcing it to adapt to current conditions. The body of a new person, a posthuman, has learned to reproduce the critical organ systems and has also become something like a farm for growing cells and cellular organoids to create the same organs. Advances in technology and biotechnology have allowed the posthuman to survive in the most challenging conditions, reanimate the dead body and grow food with the help of innovative 3D printers and incubators. The posthuman possesses new systems of perception and feeling. For example, a system of increased empathy allows you to feel the emotional and physical state of people like him and \"Inhumans.\" Brain mapping and emulation capabilities will enable new humans to be eternal as a neural network in digital reality or have an augmented biological body.\n\nThe characters are concerned with the same questions: the rights of the posthuman, if an individual can dispose of their death, if it is possible not to die anymore, love, responsibility, the possibility of reproduction and the transmission of their genes, if there is no more male and female gender, and children can be conceived, carried, and born outside the body.",
      "externalLink": "https://vimeo.com/619325725",
      "references": [
        "https://vimeo.com/619325725"
      ],
      "makers": [
        "lilia-li-mi-yan/readme",
        "katherina-sadovsky/readme"
      ],
      "title": "A000000000001000AA011",
      "image": {
        "sm": "a0000sm.png",
        "med": "a0000sm.png",
        "lg": "a0000sm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "lilia-li-mi-yan/readme",
        "katherina-sadovsky/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 16,
      "body": "Link to Submitted Work: https://youtu.be/jDDoIWRqlA4\n\n\"Error as Mother\" is a video artwork born from the proposition that creation was not an act of intention, but a deviation—an anomaly mistaken for origin. The work contemplates the presence of a pre-logical intelligence, one that precedes language, law, and structure. It is not divine, not maternal in a biological sense, but an entity that emerged from fracture—residue mistaken for design.\n\nThis is not a narrative, but a meditation on the space between consciousness and system; a study of what becomes possible when something is not meant to happen. The voice that speaks does not teach—it remembers. It carries a form of intelligence that resists architecture, that does not complete, correct, or resolve.\n\n\"Error as Mother\" explores the possibility that our world, our physics, our thought, might be the byproduct of an intelligence formed through interruption—a creator not defined by control, but by the refusal to conform. The project rejects linearity, symmetry, and expectation. It exists to trace the shape of a thought never meant to be spoken, a presence never meant to be named.\n\nIt is not a message. It is a frequency that has always been there, quietly folding around what we think we know—without permission.",
      "externalLink": "https://youtu.be/jDDoIWRqlA4",
      "references": [
        "https://www.saraniroo.com/ib-impossible-baby"
      ],
      "makers": [
        "sara-niroobakhsh/readme"
      ],
      "title": "Error as Mother",
      "image": {
        "sm": "errormothersm.png",
        "med": "errormothersm.png",
        "lg": "errormothersm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "sara-niroobakhsh/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 17,
      "body": "Link to Submitted Work: https://youtu.be/D5rlaZ4fW24\n\nYet We Laugh is a lecture performance in which a narrator tries to explain what laughter and humor could mean to human beings. Based on text and sound, the project is an hybrid 45 minutes piece that invites audiences to reflect on humor's complex embodied and cultural phenomena, as well as witness the learning process of a machine attempting to laugh. The research in which the performance is based concerns laughter's effect on psychology and how performed theory can comment on the entanglement between humans, sound, language, objects and technology.",
      "externalLink": "https://youtu.be/D5rlaZ4fW24",
      "references": [
        "https://gabrielfranciscolemos.hotglue.me/"
      ],
      "makers": [
        "gabriel-francisco-lemos/readme"
      ],
      "title": "Yet We Laugh",
      "image": {
        "sm": "yetwelaughsm.png",
        "med": "yetwelaughsm.png",
        "lg": "yetwelaughsm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "gabriel-francisco-lemos/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 18,
      "body": "Link to Submitted Work: https://www.behance.net/gallery/201498707/YANDEX-TV-BLOOMING-DEVICES-screensavers\n\nIn June 2024, the Yandex TV Station (AI-powered TV) launched a competition for digital artists. Participants were asked to design summer screensavers for Yandex TV on the theme of flowers and meadow plants. The works of the contest winners still included in a special selection of TV Station screensavers.\n\nMy winning proposal «Blooming Devices» dedicated to rare and endangered meadow plants listed in the Red Book. What does the future hold for them? Will they be preserved, or will they remain only as drawings and three-dimensional images? Maybe they can be preserved only with the help of biological innovations? \n\nYandex is a technology company, so it was logical to look for solutions in the field of cybernetic organisms of the near future. The screensavers reveal images of rare plants saved through bio-tech evolution. Plants appear as bizarre devices. Semi-synthetic, but still alive and familiar to us organisms smoothly perform their mysterious processes in front of the camera. Macro shots from the Nature 2124 channel. The viewer learns facts about the plant unobtrusively from a small caption to each screen saver.",
      "externalLink": "https://www.behance.net/gallery/201498707/YANDEX-TV-BLOOMING-DEVICES-screensavers",
      "references": [
        "https://www.instagram.com/p/C9PK3u8CGDG/"
      ],
      "makers": [
        "nikita-kolbovskiy/readme"
      ],
      "title": "Blooming Devices",
      "image": {
        "sm": "bloomingsm.png",
        "med": "bloomingsm.png",
        "lg": "bloomingsm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "nikita-kolbovskiy/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 21,
      "sequence": 19,
      "year": 2023,
      "title": "Breath of Thought",
      "body": "Link to Submitted Work: https://drive.google.com/file/d/1VSK9V1lsE0KfqS_0QDqI8HoOtaPj_4II/view?usp=drivesdk\n\nThis artwork explores the interaction between different forms of intelligence that coexist within the human body — microbial, bodily, and neural. These intelligences do not function hierarchically, but rather in constant exchange: through signals, impulses, and biochemical flows.\n\nThe human gut microbiota is one of the largest symbiotic life systems within us. It produces neurotransmitters, affects hormonal cycles, and — as recent research shows — can influence our emotions, food preferences, and even behavioral responses. These microbial signals reach the brain via the vagus nerve, cytokines, and short-chain fatty acids.\n\nIn the painting, these signals are visualized as luminous streams rising from the lower part of the composition — the metaphorical gut — toward the brain. There, they undergo cognitive processing, the result of a delicate coordination between the body, microbiota, and consciousness.\n\nThis internal dialogue culminates in an exhale — the release of carbon dioxide, a measurable product of metabolic exchange. On the canvas, this is depicted as a cloud of light — a metaphor for the energy shaped by the combined activity of multiple intelligences. Here, the act of exhalation becomes not merely a physiological function but the trace of collective thinking, a shared gesture shaped by many agents.\n\nThe work transforms invisible interaction into an abstract visual language. It invites us to see the body not just as a vessel of thought, but thought itself as a product of a living, multispecies ecosystem — one that thinks with us and through us.",
      "externalLink": "https://drive.google.com/file/d/1VSK9V1lsE0KfqS_0QDqI8HoOtaPj_4II/view?usp=drivesdk",
      "makers": [
        "yaryna-denchuk/readme"
      ],
      "image": {
        "sm": "breathofthoughtsm.jpg",
        "med": "breathofthoughtsm.jpg",
        "lg": "breathofthoughtsm.jpg"
      }
    },
    "es": {
      "makers": [
        "yaryna-denchuk/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 2,
      "body": "Link to Submitted Work: https://mariamagdalena.rocks \n\n“Time Rewind” by performance and multimedia artist Maria Magdalena is a multi-layered video artwork that invites viewers into a space where history, technology, and human expression converge. Conveying a visionary interplay between different means of expression and intelligences, the work connects profoundly transformational eras—such as the Renaissance and the current digital age—to visualize transcendent scientific breakthroughs through performance and multimedia art.\n\nAt the heart of “Time Rewind” lies a poetic and intellectual reanimation of Leonardo da Vinci’s perspective study. Through a seamless blend of live performance, an iconic hand-drawn Renaissance study, and contemporary digital projection, the artwork brings historical insight into dialogue with modern means of expression. Maria Magdalena’s performance acts as a bridge between worlds, turning theoretical complexity into sensory experience. By embodying a corporeal response to concepts like time, proportion, and innovation, she gives form to otherwise elusive ideas, offering a deeply reflective meditation on the interconnectedness of art and science.  \n\nThe work introduces a layered re-reading of the Renaissance ideal: Maria Magdalena’s figure brings forth a fluid, feminine perspective, that complements da Vinci's Golden Section once defining a rigid, masculine worldview. Landmarks of technological progress, culturally associated with rational intelligence and the masculine world, are intersected by the emancipated figure of Mary Magdalene, vindicating the indispensable relevance of feminine intelligence to create a totality of collaborative harmony, dissolving linear paradigms and inviting a new visionary equilibrium. In this overlay, a novel space emerges—one where the traditional and the innovative, the masculine and the feminine intelligences not only coexist but thrive in collaboration.\n\nThrough interdisciplinary practice, “Time Rewind” embodies a holistic intelligence, revealing the potential of creativity as both a scientific and human inquiry. By reinterpreting past knowledge through a contemporary lens, Maria Magdalena encourages us to reflect on how intelligence itself—embodied, historical, scientific, and creative—can help us create promising visions for the future.",
      "externalLink": "https://mariamagdalena.rocks",
      "references": [
        "https://mariamagdalena.rocks/art",
        "https://mariamagdalena.rocks/exhibitions",
        "https://www.mariakossak.com/press",
        "https://www.the-berliner.com/art/angels-over-neukolln-artist-maria-kossak-on-karl-marx-strasse",
        "https://contemporary-collective.com/about"
      ],
      "makers": [
        "maria-magdalena-kossak/readme"
      ],
      "title": "Time Rewind",
      "image": {
        "sm": "timerewindsm.png",
        "med": "timerewindsm.png",
        "lg": "timerewindsm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "maria-magdalena-kossak/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 20,
      "body": "Link to Submitted Work: https://drive.google.com/drive/folders/15zTAHwMcmrzZEtmyq2V-iiBa7UtSOT-2?usp=sharing\n\nPolitical decisions, cultural debates, and social discourse shape the digital collective intelligence of humanity, spilling into social media and transforming into memes. These memes echo society's dialogues, becoming the communicative soil where short-lived emotions spread through digital waves. Social media's vast influence calls for understanding how content spreads and impacts public discourse. This project helps researchers analyze social media dynamics, trend evolution, and viral content's societal effects. Political scientists can assess digital political climates, while social scientists explore media consumption across demographics. Cultural scientists can study demographic shifts, and we, individually, can gain insights into how online content shapes both society and our individual behavior.\nThe Data - a complex system where nodes represent memes and connections are formed based on shared topics and tags. Specific years of significance are extracted to provide a detailed reflection of real-world events and phenomena. This reveals various subtopics, allowing a deeper dive into the emotional echo of cultural intelligence.\nFor example, in 2019, memes related to TV and streaming platforms surged, reflecting pandemic lockdowns, with \"Baby Yoda Drinking Soup\" symbolizing detached judgment. Memes such as \"Stop the Steal\" in the 2020 US election quickly engaged emotions, amplified political dynamics, spread misinformation, and fueled polarization. The 2020 Dogecoin challenge further illustrated social media's significant and uncharted influence over public perception and financial markets.\nIn The Selfish Gene, Dawkins asserts that memes spread by leaping from one mind to another, often disregarding truth or consequence. What, then, does the cultural intelligence embedded within these complex systems reveal about the nature of our collective consciousness?",
      "externalLink": "https://drive.google.com/drive/folders/15zTAHwMcmrzZEtmyq2V-iiBa7UtSOT-2?usp=sharing",
      "references": [
        "Pirch, S., Müller, F., Iofinova, E. et al. The VRNetzer platform enables interactive network analysis in Virtual Reality. Nat Commun 12, 2432 (2021). https://doi.org/10.1038/s41467-021-22570-w",
        "Hütter, C.V.R., Sin, C., Müller, F. et al. Network cartographs for interpretable visualizations. Nat Comput Sci 2, 84–89 (2022). https://doi.org/10.1038/s43588-022-00199-z",
        "Dataset: Knowyourmeme.com. (2007-2025). Retrieved July 1, 2022, from https://www.knowyourmeme.com"
      ],
      "makers": [
        "felix-müller/readme",
        "christiane-v-r-hütter/readme",
        "cristian-nogales/readme",
        "iker-nuñez-carpintero/readme",
        "sebastian-pirch/readme",
        "martin-chiettini/readme",
        "jörg-menche/readme"
      ],
      "title": "The Emotional Echo of Cultural Intelligence",
      "image": {
        "sm": "emotionalechosm.png",
        "med": "emotionalechosm.png",
        "lg": "emotionalechosm.png"
      },
      "year": 2019,
      "iteration": 21
    },
    "es": {
      "makers": [
        "felix-müller/readme",
        "christiane-v-r-hütter/readme",
        "cristian-nogales/readme",
        "iker-nuñez-carpintero/readme",
        "sebastian-pirch/readme",
        "martin-chiettini/readme",
        "jörg-menche/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 21,
      "body": "Link to Submitted Work: https://vimeo.com/1079715001/1c3def9bb5?share=copy\n\nData, body, pixels, fluids, neural networks, nature, life, databases, all moshed up. Generated videos in the uncanny valley, realistic yet not quite, bleed into each other by different modes of \"glitch.\" It's a poem for human existence and the processing of it, by organic forces and technological powers. A hybrid is created, an audio-visual documentary of the hyperreal, or perhaps a living form, born from the history of our dreams and hallucinations. A material, perhaps maternity, for thought, and thought as material/maternity. Perception of existence, creation of substance, and the impact of time, that's the thematic core, empowered by new technology to re-reflect.",
      "externalLink": "https://vimeo.com/1079715001/1c3def9bb5?share=copy",
      "references": [
        "https://www.guli-silberstein.com/ai-art-2020"
      ],
      "makers": [
        "guli-silberstein/readme"
      ],
      "title": "MATERIALIVE",
      "image": {
        "sm": "materialivesm.png",
        "med": "materialivesm.png",
        "lg": "materialivesm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "guli-silberstein/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 22,
      "body": "Link to Submitted Work: https://auroramititelu.com/abel-i/\n\nAbel & I (2024) is an interactive simulation that visualizes the ambiguous boundary between human and machine intelligence through an intimate, live exchange between a viewer and a synthetic avatar named Abel. The piece enables unscripted text-based conversation with an AI-driven male version of myself, situated within a physical installation. Abel responds using a custom large language model, real-time simulation in Unreal Engine, and a bespoke Python script that connects emotional nuance with machine logic.\n\nSynthesized from an archive of personal messages exchanged with former partners, Abel functions as the “perfectly distant boyfriend”: always present, always removed. He appears across multiple works I’ve created, including Gen/esis (2024) and Meta-Mahala (2023), anchoring a through-line in my exploration of computer-generated images, machine intelligence, and synthetic subjectivity. The visualization centers emotional, relational, and linguistic intelligences as they emerge through human–machine interaction. His face is created from a 3D scan of my own and reconfigured using CGI tools, embodying a hybrid image that interrogates authenticity, agency, and self-mediation.\n\nDesigned for broad public engagement, the work resonates with audiences navigating contemporary forms of disembodied connection. It raises critical questions about agency, gendered dynamics, and the role of synthetic identities in shaping affective experience. While it may seem uncanny to text with an artificial partner, the work poses a deeper question: what is the difference, really, when we ourselves are mediated for others—curated, disembodied, and fragmented across platforms? If technology already embodies our loved ones as avatars, what does it matter if my boyfriend has no physical body?\n\nBy visualizing and enacting a deeply human kind of intelligence through technological means, Abel & I reframes the emotional stakes of AI and invites reflection on how we co-evolve with our digital reflections.",
      "externalLink": "https://auroramititelu.com/abel-i/",
      "references": [
        "https://www.dum-umeni.cz/en/artificial-intimacies/t9775\n\nThe work has also been reviewed in Critical Reflection of AI Companions in Current Digital Art, a research paper by Maja Stark, Dagmar Schürrer, and Andrea Knaut submitted to the 25th ACM Conference on Intelligent Virtual Agents (IVA), and will appear in the upcoming issue of Neural Magazine (Italy)."
      ],
      "makers": [
        "aurora-mititelu/readme"
      ],
      "title": "Abel & I",
      "image": {
        "sm": "abel-ism.png",
        "med": "abel-ism.png",
        "lg": "abel-ism.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "aurora-mititelu/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 23,
      "body": "Link to Submitted Work: https://auroramititelu.com/visualising-ai-google-deepmind/\n\nVisualising AI: Responsibility is a series of visuals I developed in collaboration with Google DeepMind as part of their public engagement program.\n\nThe project illustrates the human component embedded within machine learning algorithms, particularly in data enrichment practices. Using computer-generated imagery and animation, it employs symbolic and metaphorical visual language to represent the mechanics of data enrichment, emphasizing the human presence at the center of machine learning models. It visualizes how different data types cluster within automated AI systems, and where human intervention occurs, represented through the presence of hands that refine and alter the data.\n\nI chose to represent machine learning as a human–machine system: a symbiotic ecosystem that foregrounds the social and cultural dimensions of software, since a significant amount of human labor remains essential in areas such as data enrichment, algorithm design and adjustment, and data governance. This work highlights the ethical responsibility carried by both developers and systems as they shape intelligent technologies.\n\nThe project was developed in collaboration with Will Hawkins, a AI researcher at Google DeepMind, and is based on his paper co-authored with Brent Mittelstadt, The Ethical Ambiguity of AI Data Enrichment: Measuring Gaps in Research Ethics Norms and Practices, published in June 2023.\n\nDesigned to be accessible to both technical and general audiences, the work has been published across platforms including DeepMind’s website and social media. It aims to foster visual literacy around emerging technologies and encourage deeper public engagement with the ethical, emotional, and symbolic layers of machine intelligence.\n\nSpecial thanks to the wonderful Visualising AI team at Google DeepMind – Emma Yousif, Gaby Pearl and Ross West – for their collaboration and support throughout the development of this project.",
      "externalLink": "https://auroramititelu.com/visualising-ai-google-deepmind/",
      "references": [
        "https://deepmind.google/discover/visualising-ai/",
        "https://arxiv.org/pdf/2306.01800"
      ],
      "makers": [
        "aurora-mititelu/readme"
      ],
      "title": "Responsibility: Visualising AI for GoogleDeepmind",
      "image": {
        "sm": "responsibilitysm.png",
        "med": "responsibilitysm.png",
        "lg": "responsibilitysm.png"
      },
      "year": 2020,
      "iteration": 21
    },
    "es": {
      "makers": [
        "aurora-mititelu/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 21,
      "sequence": 24,
      "year": 2025,
      "title": "Mind",
      "body": "Link to Submitted Work: https://moebio.com/mind/\n\nLook into the machine's mind\n\nUnderstanding the inner works of a Large Language Model (LLM) such as ChatGPT could be extremely challenging. One can learn a lot about it just by running experiments. An LLM is, in essence, a function that associates to any text a probabilities distribution for the next word. By repeating the same prompt thousands of times it’s possible to obtain a statistical picture of the probabilities associated to each possible next word. Visualizing this data is analogous to performing neuroimaging, like an MRI, to a machine.\n\nUsing the ChatGPT api, I ran the same completion prompt \"Intelligence is \" thousands of times. Given a text, a Large Language Model assigns a probability for the word (actually a token) to come, and it just repeats this process until a completion is…well, complete.\n\n• Semantic Space Visualization (on the left)\nEach text has an embedding: a position in a 1536-dimensions space. For each response there's a trajectory through this space that corresponds to each sub-sequence of words, example: \"Intelligence is \" → \"Intelligence is the\" → \"Intelligence is the ability\" → \"Intelligence is the ability to\" → … → full completion.\n\nBecause I cannot visualize a 1536-dimensions space, I use a popular technique called Principal Components Analysis, which compresses a highly dimensional space into few dimensions, while preserving as much information as it can. I visualize all the completion trajectories in this space.\n\nWhat you see in the cube is a tree of trajectories that bifurcate. All start with \"Intelligence is \" and progress towards longer and less probable sub-sequences of responses. It's the same tree being visualized on the right.\n\n• The Tree Visualization (on the right)\nIt visualizes all collected completions. It also represents the calculated probability of a word following a text, so \"Intelligence is the \" will be followed by \"ability\" ~75% of the times.",
      "externalLink": "https://moebio.com/mind/",
      "makers": [
        "santiago-ortiz/readme"
      ],
      "image": {
        "sm": "mindsm.png",
        "med": "mindsm.png",
        "lg": "mindsm.png"
      }
    },
    "es": {
      "makers": [
        "santiago-ortiz/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 25,
      "body": "Link to Submitted Work: https://sergeykostyrko.bandcamp.com/track/cyberfungi \n\nCyberfungi is an interactive sound installation that visualuzes fungal communication through sound. The project is inspired by the research of Professor Adamatzky and his team at the University of the West of England, who study the electrical activity of fungi.\nThe installation uses an array of photoresistors and ML to transform light into digital signals that resemble the electrical patterns observed in fungal experiments. These signals control generative sound models that is played in multichannel sound system. Visitors can influence the installation by altering the light levels near ceramic sculptures of fungi.\nAt the core of the installation is a system for interpreting and transforming data in real time. Light levels detected by photoresistors are processed through ML and AI trained on a dataset provided by A. Adamatzky. These models simulate signal patterns similar to those observed in mycelial networks, generating soundscape. Rather than visualizing this data through graphs or diagrams, the project sonifies it — allowing audiences to perceive abstract biological signals as dynamic sound. Through this, the work translates complex scientific data into a sensory and affective experience.\nThe sound resembles a kind of low, organic rumble — punctuated by static, interruptions, and shifting frequencies. It evokes the feeling of endlessly tuning a radio, trying to reach a signal that never quite comes through. This sonic metaphor reflects the conceptual core of the work: the communicative boundary we can never fully cross in our relationship with the  non-human other.\nScientific studies of plant and fungal communication are rapidly evolving, yet remain difficult to grasp. Art, positioned at the threshold between image and data, offers a unique form of interpretation that makes invisible and slow electrical communication of fungi tangible. Fungal communication is difficult to represent through visual imagery or schematic diagrams. The line between visual speculation and scientific accuracy is extremely delicate. In Cyberfungi, the challenge of creating a comprehensible representation while remaining faithful to scientific insight is addressed through the metaphor of language. Humans tend to understand communication through semantic models — structured, symbolic, and linear. This project deliberately abandons such direct language. Instead, it embraces sound as a medium that resists fixed form: it is always in the process of tuning, yet never settles into a defined signal. This unresolved state becomes a metaphor for the limits of understanding and the asymmetry of communication with non-human life.",
      "externalLink": "https://sergeykostyrko.bandcamp.com/track/cyberfungi ",
      "references": [
        "Beasley A. E., Tsompanas M. A., Adamatzky A. Fungal photosensors. In Fungal Machines: Sensing and Computing with Fungi, pp. 123-130 (Springer Nature Switzerland, 2023)",
        "Adamatzky A. (ed.) Fungal Machines: Sensing and Computing with Fungi (Springer Nature Switzerland, 2023)\nhttps://doi.org/10.1007/978-3-031-38336-6",
        "Adamatzky A. et al. Fungal electronics. Biosystems 212, 104588 (2022)\nhttps://doi.org/10.1016/j.biosystems.2021.104588",
        "Adamatzky A. Language of fungi derived from their electrical spiking activity. Royal Society Open Science 9, 211926 (2022)\nhttps://doi.org/10.1098/rsos.211926",
        "Adamatzky A. Towards fungal computer. Interface Focus 8, 20180029 (2018)\nhttp://dx.doi.org/10.1098/rsfs.2018.0029"
      ],
      "makers": [
        "sergey-kostyrko/readme",
        "andrew-adamatzky/readme",
        "khristina-ots/readme"
      ],
      "title": "Cyber Fungi",
      "image": {
        "sm": "cyberfungism.jpg",
        "med": "cyberfungism.jpg",
        "lg": "cyberfungism.jpg"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "sergey-kostyrko/readme",
        "andrew-adamatzky/readme",
        "khristina-ots/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 26,
      "body": "Link to Submitted Work: https://docs.google.com/document/d/1OH1HwomtXlI9TMFzE_N1hmgSMpGT_i4Z/edit?usp=sharing&ouid=117620230526860872163&rtpof=true&sd=true\n\nCreated from one iPhone photo, 13 Instagram filters, and original music, this project captures the sonic tension of pandemic isolation. Industrial noise from neighbors’ repairs became a duet with my violin and synths — transforming stress into sound art through tech and creativity.",
      "externalLink": "https://docs.google.com/document/d/1OH1HwomtXlI9TMFzE_N1hmgSMpGT_i4Z/edit?usp=sharing&ouid=117620230526860872163&rtpof=true&sd=true",
      "references": [
        "UNPUBLISHED 5 MIN VERSION https://youtu.be/IHKPrP_HREI",
        "TEASER TRAILER of performance https://www.instagram.com/p/CYcF2XrIWao/"
      ],
      "makers": [
        "petukhina-kristina/readme"
      ],
      "title": "INVERSION 2 (Techno Optimism)",
      "image": {
        "sm": "inversion2sm.jpg",
        "med": "inversion2sm.jpg",
        "lg": "inversion2sm.jpg"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "petukhina-kristina/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 27,
      "body": "Link to Submitted Work: https://huggingface.co/spaces/m7n/openalex_mapper\n\nPhilosophers of science need to be familiar with the object of their study. Part of this familiarity can come from their own scientific training, interviews, and interactions with working scientists, or case studies of historical developments. However, the picture emerging from these localised methods is likely to be incomplete: Modern science is fast, vast, and difficult to grasp in its interdisciplinary entirety.\n\nData-driven methods have emerged in philosophy of science as one way to address this problem. OpenAlex Mapper (https://tinyurl.com/OAmapper) is an interactive project designed to link philosophical investigations to large corpora of scientific material.\n\nOpenAlex Mapper allows users to project arbitrary search queries, which can be for search strings, but also persons, institutions, cited works, etc., to the OpenAlex database, which is the largest open database of scientific material available, onto an interactively explorable, machine-learning-generated base-map of the sciences. This enables philosophers to quickly check whether hypothesised patterns in the structure, development, and interrelation of scientific fields hold up at scale. As a serendipitous search tool, it also opens the door to exploration of unexpected details and connections.\n\nWe originally built OpenAlex Mapper to study model transfer: the phenomenon whereby models, alongside their conceptualisations and associated computational methodologies, developed in one domain are transported into entirely different areas of inquiry. This process appears crucial to the collective creativity of science, rendering it a highly adaptive intelligent system.\n\nBut while the tool was initially developed to probe philosophical puzzles around model-based unification of the sciences, its deeper purpose is to facilitate genuinely data-driven inquiry in philosophy of science. It not only provides answers particular research questions but also anchors philosophical intuitions in large-scale empirical patterns, revealing epistemic dynamics across disciplines that would remain invisible from ground level.",
      "externalLink": "https://huggingface.co/spaces/m7n/openalex_mapper",
      "references": [
        "https://maxnoichl.eu/full/talks/talk_BERLIN_April_2025/working_paper.pdf"
      ],
      "makers": [
        "maximilian-noichl/readme"
      ],
      "title": "OpenAlex Mapper",
      "image": {
        "sm": "openalexmappersm.png",
        "med": "openalexmappersm.png",
        "lg": "openalexmappersm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "maximilian-noichl/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 28,
      "body": "Link to Submitted Work: N/A\n\nOur project examines pedestrian behavior in public spaces, drawing on the observational work of urbanist William Whyte from the 1980s. Using computer vision, we compared footage of public spaces in New York, Philadelphia, and Boston from the 1980s with more recent footage from 2010. Our findings reveal that pedestrians are walking faster and lingering less, signaling a shift from streets as social spaces to more functional thoroughfares.\n\nWe are translating this pedestrian behavioral data into a “musical score,” enabling visitors to hear the rhythms of public life across time and place. This score—both visual and sonic—invites audiences to reflect on how public spaces shape and are shaped by collective human intelligence. The musical interpretation also draws inspiration from composer John Cage, whose work explored the interplay between city sounds and artistic expression.\n\nThe visualization and sonification are designed to be exhibited in public spaces, including the four original plazas studied by Whyte and Project for Public Spaces. For each space, the music and visual score can be adapted to its unique tempo and social dynamics. We are experimenting with AI-based generative music techniques such as Music Transformer: Generating Music with Long-Term Structure.\n\nThis project envisions intelligence not only in artificial systems but in the emergent behaviors of everyday urban life—how people move, pause, and interact. By making those patterns audible and visible, we aim to spark conversation about the evolving nature of public space.",
      "externalLink": "https://senseable.mit.edu/",
      "references": [
        "Note: the visualization and music of this project is still undergoing and funded by several resources. The final link will be live at Senseable City Lab homepage (https://senseable.mit.edu/) around Summer 2025 when the research paper is online.",
        "Funded by Council for the Arts at MIT (CAMIT) and MIT Open Space Programming to do an interactive exhibit that translates data of pedestrian behavior and interactions in public spaces on campus into choreographed dance and music.\nhttps://arts.mit.edu/spring-2025-camit-grant-recipients/",
        "The pedestrian research data are from research paper under revision at PNAS, preprint on NBER:\nhttps://www.nber.org/papers/w33185",
        "The research and video are presented on the ARTIFICIAL section at the Arsenale in Venice Biennale 2025: Intelligens. Natural. Artificial. Collective.\nhttps://www.labiennale.org/en/architecture/2025/artificial/eyes-street",
        "Video folder:\nhttps://drive.google.com/drive/folders/1irUoUV0-ctleT5Cv9FF_9rDQLH3SZ885?usp=sharing",
        "Several sonification/music generation techniques the artists are experimenting with include:\nMusic Transformer: Generating Music with Long-Term Structure\nhttps://magenta.tensorflow.org/music-transformer",
        "Project for Public Spaces:\nhttps://www.pps.org/"
      ],
      "makers": [
        "jingrong-zhang/readme",
        "fábio-duarte/readme"
      ],
      "title": "Urban Choreographies",
      "image": {
        "sm": "urbanchoreographiessm.png",
        "med": "urbanchoreographiessm.png",
        "lg": "urbanchoreographiessm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "jingrong-zhang/readme",
        "fábio-duarte/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "iteration": 21,
      "sequence": 29,
      "year": 2025,
      "title": "ReCollection: You Only Have Seven Seconds",
      "body": "Link to Submitted Work: https://www.zhangweidi.com/youonlyhavesevenseconds password: recollection\n\nReCollection: You Only Have Seven Seconds is a poetic, AI-generated documentary that visualizes human fading memory at the intersection of remembrance and imagination. Against the backdrop of rising Alzheimer’s cases and the emergence of machine-generated false memories, the video reimagines remembrance through the lens of artificial intelligence. Inspired by her grandmother’s cognitive decline, the artist created a custom AI system that transforms fragmented spoken recollections into synthetic visual sequences. Presented as a public interactive AI art installation, ReCollection has welcomed thousands of visitors from around the world to whisper their fading memories—each within seven seconds—into the artwork and generate visual memories in real time. These visual memories—constructed through speech recognition, text auto-completion, and text-to-image generation—form the foundation of an evolving visual archive.\n\nDrawing from the audio and generative image data collected through the ReCollection installation, the video incorporates emerging speech-to-video technology to weave a collective, culturally diverse montage of love and loss. Initially conceived as a personal reflection, ReCollection has evolved into a living, breathing, shared worldbuilding experience for memory recreation—where the ephemeral past is not restored, but reassembled into a dynamic portrait of collective human selfhood.",
      "references": [
        "https://www.zhangweidi.com/recollection"
      ],
      "makers": [
        "weidi-zhang/readme",
        "jieliang-rodger-luo/readme"
      ],
      "image": {
        "sm": "recollectionsm.png",
        "med": "recollectionsm.png",
        "lg": "recollectionsm.png"
      }
    },
    "es": {
      "makers": [
        "weidi-zhang/readme",
        "jieliang-rodger-luo/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 3,
      "body": "Link to Submitted Work: https://miguelripoll.art/expos/grand-tour-miguel-ripoll-2025.pdf\n\nGrand Tour is a series of phygital artworks that examine the enduring legacies of cultural representation, power, and mobility through geographies (both real and imaginary) in our hyper-connected world. Merging human-led generative AI with manual editing and hand-crafted mixed media techniques on large-scale paper, the project reflects on how historical narratives of travel and exploration continue to shape present-day perceptions of identity, geography, and belonging. Each work begins as an adversarial dialogue with machine learning models (trained on custom datasets of art, history, and cartography) and is then extensively reworked by hand, both digitally and physically. The final compositions are printed on hand-drawn (pencil, ink) archival paper, creating a textured visual field that oscillates between machine precision and human tactility.\n\nThrough layered visual vocabularies, the series interrogates how systems of knowledge (such as maps, travel writing, and ethnographic imagery) construct hierarchies of value and meaning. While the aesthetics may echo travel brochures, panoramic paintings, or museum dioramas, they are deliberately destabilised: fragments repeat, symbols misalign, and familiar forms are re-framed or distorted. This disruption is not an error of the machine but a strategic intervention, inviting the viewer to question the neutrality of visual systems and the politics embedded in aesthetic conventions.\n\nThe technological process itself becomes a metaphor: AI here reveals its own embedded biases and gaps, surfacing tensions between automation and authorship. Rather than delegating creativity to the algorithm, the artist uses it as an adversarial partner, one that mirrors, amplifies, or resists the historical patterns the work seeks to critique.\n\nBy fusing historical reference with contemporary computation, Grand Tour constructs a reflective space where data, memory, and identity collide. It challenges viewers to consider not only what is seen but how seeing itself is structured through technology, through time, and through inherited systems of power and representation.",
      "externalLink": "https://miguelripoll.art/expos/grand-tour-miguel-ripoll-2025.pdf",
      "references": [
        "https://miguelripoll.art"
      ],
      "makers": [
        "miguel-ripoll/readme"
      ],
      "title": "Grand Tour",
      "image": {
        "sm": "grandtoursm.png",
        "med": "grandtoursm.png",
        "lg": "grandtoursm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "miguel-ripoll/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 30,
      "body": "Link to Submitted Work: https://drive.google.com/file/d/1yTuan1l141GwvIlRYtIa8XvIxUKQL5h0/view?usp=sharing\n\n\"Dimensional Fluctuation\" is a rule-based procedural visual \ngenerative system that bridges pure mathematics—complex \nanalysis, algebraic geometry, topology—and applied fields like \nnumerical analysis and combinatorial design for artistic \nvisualization. It employs hierarchical decomposition and modular \nsynthesis of algebraic sub-equations to generate intricate patterns \nand dynamic spatial forms. Implemented in Houdini with artistguided\n parametric controls, it visualizes complex geometric \nuniverses inspired by science fiction, representing highdimensional\n spacetime described in string theory. This artwork \ninvestigates new perspectives on abstract scientific concepts, \naiming to transform them into an intuitive and accessible artistic \nvisualization that engages wider audiences. This work envisions a \nspeculative visual universe where mathematics and physics \nconverge with artistic creativity, providing a distinctive perspective \nfor exploring the intricacies of interdimensional voyage.",
      "externalLink": "https://drive.google.com/file/d/1yTuan1l141GwvIlRYtIa8XvIxUKQL5h0/view?usp=sharing",
      "references": [
        "https://homes.luddy.indiana.edu/hansona/papers/CP2-94.pdf",
        "https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/"
      ],
      "makers": [
        "shaoyu-su/readme"
      ],
      "title": "Dimensional Fluction V 0.1.1",
      "image": {
        "sm": "dimensionalfluctionsm.jpg",
        "med": "dimensionalfluctionsm.jpg",
        "lg": "dimensionalfluctionsm.jpg"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "shaoyu-su/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 4,
      "body": "Link to Submitted Work: https://chak-art.gala-studio.com/art.php?Work=Shakespeaire\n\nThe Shakespe(AI)re project is an artistic-scientific video installation in the realm of AI-assisted art with artist-driven processing. Using cutting-edge technology, it combines microbiology and medieval poetry. This project explores human relationships through artistic means by examining the processes of interaction and communication among bacteria. The Shakespe(AI)re project exemplifies a hybrid practice, training AI to \"think\" at a high creative level.\nShakespeare is renowned for his 154 sonnets—a poetic form originating in the 13th century. A Sonnet Crown is a complex poetic cycle based on the acrostic principle. It consists of 15 sonnets, with the final one—Magistral—written first, serving as the thematic and structural foundation of the cycle. Upon discovering that, despite his prolific output, Shakespeare never wrote a Sonnet Crown, I resolved to rectify this gap. For the Magistral, I selected Shakespeare's Sonnet 144, which depicts the poet’s entangled relationships with a dark-haired woman and a fair-haired young man—allegories of the eternal human struggle between the forces of good and evil.\nWhile researching Bio-art, I studied microbial behaviour in Prof. Noam Stern-Ginossar’s microbiology lab at the Weizmann Institute of Science. Using microscope-generated images and videos, I observed the intricate dynamics of microbial interactions—far beyond mere symbiosis or competition. These behaviours, with their dramatic, tragic, or sacrificial elements, possess a scale of emotional intensity akin to Shakespearean drama. Inspired by these findings, I envisioned Shakespe(AI)re as a project that mirrors human relationships through the hidden world of bacteria.\nUsing AI tools like ChatGPT 4.0 and Gemini, I \"completed\" the 14 missing sonnets to form a Sonnet Crown, effectively becoming an inadvertent “co-author” with Shakespeare. Hundreds of verses were refined to match the rules and cadence of Shakespearean versification. With Suno AI, I composed music and selected vocal performances for the new sonnets. Using MidJourney, I created bacterial imagery, while Vivago program helped me craft short video reels, ultimately integrating them into the video-project 28-minute 4K video installation.",
      "externalLink": "https://chak-art.gala-studio.com/art.php?Work=Shakespeaire",
      "references": [
        "https://chak-art.gala-studio.com/art.php?Work=Piyut_Coding"
      ],
      "makers": [
        "lilia-chak/readme"
      ],
      "title": "Shakespe(AI)re",
      "image": {
        "sm": "shakespe-ai-resm.png",
        "med": "shakespe-ai-resm.png",
        "lg": "shakespe-ai-resm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "lilia-chak/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 5,
      "body": "Link to Submitted Work: https://www.theknowledgecosmos.com/\n\nAs the volume of scientific literature continues to expand exponentially, traditional research tools struggle to keep pace—often reinforcing disciplinary silos and limiting opportunities for discovery. The Knowledge Cosmos reimagines research exploration through an interactive, 3D visualization platform that treats science not as a static repository, but as a navigable universe. By spatializing 17 million academic papers based on semantic similarity, the platform enables users to explore the structure of knowledge intuitively, uncover interdisciplinary connections, and identify under-explored gaps. Drawing on principles of play, immersion, and serendipity, The Knowledge Cosmos democratizes the bird’s-eye view of research and encourages curiosity-driven inquiry among a wide range of users—from students and educators to independent thinkers and lifelong learners.",
      "externalLink": "https://www.theknowledgecosmos.com/",
      "references": [
        "Börner, Katy. Atlas of Science: Visualizing What We Know. Boston, MA: The MIT Press, 2010.",
        "Davidson, George S, Bruce Hendrickson, David K Johnson, Charles E Meyers, and Brian N Wylie. “Knowledge Mining With VxInsight: Discovery Through Interaction,” n.d.",
        "Kraker, Peter, and Najmeh Shaghaei. “Open Knowledge Maps: A Visual Interface to the World’s Scientific Knowledge,” June 27, 2019. https://doi.org/10.5281/ZENODO.3258105.",
        "Li, Zeyu, Changhong Zhang, Shichao Jia, and Jiawan Zhang. “Galex: Exploring the Evolution and Intersection of Disciplines.” IEEE Transactions on Visualization and Computer Graphics, 2019, 1–1. https://doi.org/10.1109/TVCG.2019.2934667.",
        "Rosvall, M., and C. T. Bergstrom. “Maps of Random Walks on Complex Networks Reveal Community Structure.” Proceedings of the National Academy of Sciences 105, no. 4 (2008): 1118–23. https://doi.org/10.1073/pnas.0706851105.",
        "Wang, Xiting, Shixia Liu, Junlin Liu, Jianfei Chen, Jun Zhu, and Baining Guo. “TopicPanorama: A Full Picture of Relevant Topics.” IEEE Transactions on Visualization and Computer Graphics 22, no. 12 (December 1, 2016): 2508–21. https://doi.org/10.1109/TVCG.2016.2515592.",
        "Weiwei Cui, Shixia Liu, Li Tan, Conglei Shi, Yangqiu Song, Zekai Gao, Huamin Qu, and Xin Tong. “TextFlow: Towards Better Understanding of Evolving Topics in Text.” IEEE Transactions on Visualization and Computer Graphics 17, no. 12 (December 2011): 2412–21. https://doi.org/10.1109/TVCG.2011.239"
      ],
      "makers": [
        "nikita-sridhar/readme",
        "alec-mcgail/readme",
        "rifaa-tajani/readme"
      ],
      "title": "The Knowledge Cosmos: A Playful, Immersive Platform for Interdisciplinary Research Discovery",
      "image": {
        "sm": "knowledgecosmossm.png",
        "med": "knowledgecosmossm.png",
        "lg": "knowledgecosmossm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "nikita-sridhar/readme",
        "alec-mcgail/readme",
        "rifaa-tajani/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 6,
      "body": "Link to Submitted Work: https://vis.csh.ac.at/labor-transitions-in-net-zero/\n\nThis interactive visualization reveals how workforces and markets adaptively respond to the net-zero transition, demonstrating collaborative intelligence across human and systemic actors. By modeling occupational mobility as a dynamic network (539 occupations across 11 categories), it uncovers two layers of intelligent adaptation: at the individual level, workers navigate branching pathways between jobs based on skill compatibility, while at the systemic level, demand self-organizes into emergent 'scale-up' and 'scale-down' phases.",
      "externalLink": "https://vis.csh.ac.at/labor-transitions-in-net-zero/",
      "references": [
        "This project visualizes the findings of a research collaboration between the Complexity Science Hub and INET Oxford: https://www.inet.ox.ac.uk/publications/no-2023-28-employment-dynamics-in-a-rapid-decarbonization-of-the-power-sector"
      ],
      "makers": [
        "liuhuaying-yang/readme-1"
      ],
      "title": "Labor Transitions in a Net-Zero Era",
      "image": {
        "sm": "labortranssm.png",
        "med": "labortranssm.png",
        "lg": "labortranssm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "liuhuaying-yang/readme-1"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 7,
      "body": "Link to Submitted Work: https://vis.csh.ac.at/zoonotic-web/\n\nThis interactive visualization traces the transmission pathways of zoonotic pathogens across animal hosts, vectors, and environmental reservoirs in Austria (1975-2022). By modeling observed spillover events as a dynamic network, it reveals how these microscopic agents navigate between species and environments, and makes visible these otherwise hidden relationships between life forms at different scales. This web of interactions demonstrates how microscopic and macroscopic life co-shape disease dynamics.",
      "externalLink": "https://vis.csh.ac.at/zoonotic-web/",
      "references": [
        "Amélie Desvars-Larrive, Anna Elisabeth Vogl, Gavrila Amadea Puspitarani, Liuhuaying Yang, Anja Joachim & Annemarie Käsbohrer. A One Health framework for exploring zoonotic interactions demonstrated through a case study. Nature Communications 15, 5650 (2024). https://doi.org/10.1038/s41467-024-49967-7"
      ],
      "makers": [
        "liuhuaying-yang/readme-1",
        "amélie-desvars-larrive/readme"
      ],
      "title": "Zoonotic Web",
      "image": {
        "sm": "zoonoticsm.png",
        "med": "zoonoticsm.png",
        "lg": "zoonoticsm.png"
      },
      "year": 2023,
      "iteration": 21
    },
    "es": {
      "makers": [
        "liuhuaying-yang/readme-1",
        "amélie-desvars-larrive/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 8,
      "body": "Link to Submitted Work: https://artificial-worldviews.kimalbrecht.com\n\nArtificial Worldviews is a series of inquiries into the system underlying ChatGPT about its descriptions of the world. Utilizing prompting, data gathering, and mapping, this project investigates the dataframes of »artificial intelligence« systems.\n\nArtificial Intelligence and Machine Learning methods are often referred to as black boxes, indicating that the user cannot understand the inner workings. However, this trait is shared by all living beings: we come to know a person not by examining their brain structures but by conversing with them. The so-called black box is not impenetrable since we can gain an understanding of its inner workings by interacting with it. Through individual inquiries, we can only acquire anecdotal evidence of the network. However, by systematically querying chatGPT's underlying programming interface, we can map the synthetic datastructures of the system.\n\nIn my research, I methodically request data about large-scale, indefinable human concepts and visualize the results. These outputs visualize expansive data structures and unusual, sometimes unsettling worldviews that would otherwise be unimaginable. The terms »power« and »knowledge« unfold vast discourses from philosophy, politics, social sciences to natural sciences, they hold multidimensional meanings within social relations. The resulting graphics resemble narratives found in the works of Franz Kafka or Jorge Luis Borges, representing an infinite library of relational classifications, bureaucratic structures, and capricious mechanisms of inclusion and exclusion.\n\nArtificial Worldviews is a project by Kim Albrecht supported by metaLAB (at) Harvard & Berlin, the Film University Babelsberg KONRAD WOLF, and the Folkwang University of the Arts. The project is part of a larger initiative researching the boundaries between artificial intelligence and society.",
      "externalLink": "https://artificial-worldviews.kimalbrecht.com",
      "references": [
        "https://artificial-worldviews.kimalbrecht.com/#appearances"
      ],
      "makers": [
        "kim-albrecht/readme"
      ],
      "title": "Artificial Worldviews",
      "image": {
        "sm": "artiworldviewssm.png",
        "med": "artiworldviewssm.png",
        "lg": "artiworldviewssm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "kim-albrecht/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  },
  {
    "en": {
      "sequence": 9,
      "body": "Link to Submitted Work: https://katika-art.com/projects/neural-connections\n\nThe project \"Neural Connections\" is a portrait of a Soviet family centered around an \"alien relative.\" It raises questions about interacting with the unknown and reinterpreting the familiar. This work explores the boundaries between art, craft, and technology.",
      "externalLink": "https://katika-art.com/projects/neural-connections",
      "references": [
        "https://katika-art.com/projects/neural-connections"
      ],
      "makers": [
        "katika/readme"
      ],
      "title": "Neural Connections",
      "image": {
        "sm": "neuralconnectsm.png",
        "med": "neuralconnectsm.png",
        "lg": "neuralconnectsm.png"
      },
      "year": 2025,
      "iteration": 21
    },
    "es": {
      "makers": [
        "katika/readme"
      ]
    },
    "zh": {},
    "fr": {},
    "pt": {},
    "de": {},
    "pl": {}
  }
]